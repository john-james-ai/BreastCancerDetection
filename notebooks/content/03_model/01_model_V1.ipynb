{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "from bcd.model.factory import ModelFactoryV1\n",
    "from bcd.model.experiment import Experiment\n",
    "from bcd.model.repo import ModelRepo\n",
    "pd.set_option('display.max_rows',999)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"01_model_V1.ipynb\"\n"
     ]
    }
   ],
   "source": [
    "# Experiment Parameters\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"01_model_V1.ipynb\"\n",
    "full_dataset = False\n",
    "project = \"BCD_V1_CBIS-DDSM\" if full_dataset else \"BCD_V1_CBIS-DDSM-10\" \n",
    "\n",
    "# Model Parameters\n",
    "force = True  # Whether to retrain if the model and weights already exist from a prior training session.\n",
    "metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "loss = \"binary_crossentropy\"\n",
    "activation = \"sigmoid\"\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "num_classes = 2\n",
    "\n",
    "# Dataset params\n",
    "dataset = \"CBIS-DDSM\" if full_dataset else \"CBIS-DDSM-10\"\n",
    "batch_size = 64 if full_dataset else 32\n",
    "input_shape = (224,224,3)\n",
    "output_shape = 1\n",
    "train_dir = pathlib.Path(\"data/image/1_final/training/training/\").with_suffix('') if full_dataset else pathlib.Path(\"data/image/1_final/training_10/training/\").with_suffix('') \n",
    "test_dir = pathlib.Path(\"data/image/1_final/test/test/\").with_suffix('')\n",
    "\n",
    "# Early stop parameters \n",
    "es_min_delta = 0.0001\n",
    "es_monitor = \"val_loss\"  # Monitor validation loss for early stopping\n",
    "es_patience = 10  # The number of epochs for which lack of improvement is tolerated \n",
    "es_restore_best_weights = True  # Returns the best weights rather than the weights at the last epoch.\n",
    "es_verbose = 1\n",
    "\n",
    "# Reduce LR on Plateau Parameters\n",
    "rlr_monitor = \"val_loss\"\n",
    "rlr_factor = 0.5\n",
    "rlr_patience = 3\n",
    "rlr_verbose = 1\n",
    "rlr_mode = \"auto\"\n",
    "rlr_min_delta = 1e-4\n",
    "rlr_min_lr=1e-10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"project\": project,        \n",
    "    \"dataset\": dataset,\n",
    "    \"model\": None,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,    \n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"loss\": loss,    \n",
    "    \"early_stop_min_delta\": es_min_delta,\n",
    "    \"early_stop_monitor\": es_monitor,\n",
    "    \"early_stop_patience\": es_patience,\n",
    "    \"early_stop_restore_best_weights\": es_restore_best_weights,\n",
    "    \"early_stop_verbose\": es_verbose,\n",
    "    \"rlr_monitor\": rlr_monitor,\n",
    "    \"rlr_factor\": rlr_factor,\n",
    "    \"rlr_patience\": rlr_patience,\n",
    "    \"rlr_verbose\": rlr_verbose,\n",
    "    \"rlr_mode\": rlr_mode,\n",
    "    \"rlr_min_delta\": rlr_min_delta,\n",
    "    \"rlr_min_lr\": rlr_min_lr\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 files belonging to 2 classes.\n",
      "Using 221 files for training.\n",
      "Found 276 files belonging to 2 classes.\n",
      "Using 55 files for validation.\n",
      "Found 649 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training DataSet (10%)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Validation DataSet (10%)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Test Set\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=es_monitor, \n",
    "                                                       min_delta=es_min_delta,\n",
    "                                                       patience=es_patience, \n",
    "                                                       restore_best_weights=es_restore_best_weights,\n",
    "                                                       verbose=es_verbose)\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=rlr_monitor,\n",
    "                                                          factor=rlr_factor,\n",
    "                                                          patience=rlr_patience,\n",
    "                                                          verbose=rlr_verbose,\n",
    "                                                          mode=rlr_mode,\n",
    "                                                          min_delta=rlr_min_delta,\n",
    "                                                          min_lr=rlr_min_lr)\n",
    "callbacks = [early_stop_callback, reduce_lr_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "repo = ModelRepo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "Remove existing runs for the project from weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find project BCD_V1_CBIS-DDSM-10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m entity \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_ENTITY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m runs \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mApi()\u001b[38;5;241m.\u001b[39mruns(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m runs:\n\u001b[1;32m      4\u001b[0m     run\u001b[38;5;241m.\u001b[39mdelete()\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/wandb/apis/paginator.py:75\u001b[0m, in \u001b[0;36mPaginator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/wandb/apis/paginator.py:62\u001b[0m, in \u001b[0;36mPaginator._load_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_variables()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQUERY, variable_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/wandb/apis/public/runs.py:133\u001b[0m, in \u001b[0;36mRuns.convert_objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m objs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find project \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run_response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    135\u001b[0m     run \u001b[38;5;241m=\u001b[39m Run(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient,\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         include_sweeps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_include_sweeps,\n\u001b[1;32m    142\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find project BCD_V1_CBIS-DDSM-10"
     ]
    }
   ],
   "source": [
    "entity = os.getenv(\"WANDB_ENTITY\")\n",
    "try:\n",
    "    runs = wandb.Api().runs(f\"{entity}/{project}\")\n",
    "    for run in runs:\n",
    "        run.delete()\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the DenseNet model\n",
    "factory = ModelFactoryV1(input_shape=input_shape, output_shape=output_shape, activation=activation)\n",
    "densenet = factory.create_densenet()\n",
    "densenet.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = densenet.alias\n",
    "densenet_experiment = Experiment(model=densenet, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "densenet_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "densenet_experiment.classification_report(data=val_ds)\n",
    "densenet_experiment.plot_confusion_matrix(data=val_ds)\n",
    "densenet_experiment.evaluate(data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the ResNet model\n",
    "resnet = factory.create_resnet()\n",
    "resnet.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = resnet.alias\n",
    "resnet_experiment = Experiment(model=resnet, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "resnet_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "resnet_experiment.classification_report(data=val_ds)\n",
    "resnet_experiment.plot_confusion_matrix(data=val_ds)\n",
    "resnet_experiment.evaluate(data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the ResNet model\n",
    "inception = factory.create_inception()\n",
    "inception.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = inception.alias\n",
    "inception_experiment = Experiment(model=inception, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "inception_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "inception_experiment.classification_report(data=val_ds)\n",
    "inception_experiment.plot_confusion_matrix(data=val_ds)\n",
    "inception_experiment.evaluate(data=test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Efficientnet model\n",
    "efficientnet = factory.create_efficientnet()\n",
    "efficientnet.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = efficientnet.alias\n",
    "efficientnet_experiment = Experiment(model=efficientnet, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "efficientnet_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "efficientnet_experiment.classification_report(data=val_ds)\n",
    "efficientnet_experiment.plot_confusion_matrix(data=val_ds)\n",
    "efficientnet_experiment.evaluate(data=test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception/Resnet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Inception_resnet model\n",
    "inception_resnet = factory.create_inception_resnet()\n",
    "inception_resnet.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = inception_resnet.alias\n",
    "inception_resnet_experiment = Experiment(model=inception_resnet, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "inception_resnet_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "inception_resnet_experiment.classification_report(data=val_ds)\n",
    "inception_resnet_experiment.plot_confusion_matrix(data=val_ds)\n",
    "inception_resnet_experiment.evaluate(data=test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Mobilenet model\n",
    "mobilenet = factory.create_mobilenet()\n",
    "mobilenet.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = mobilenet.alias\n",
    "mobilenet_experiment = Experiment(model=mobilenet, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "mobilenet_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "mobilenet_experiment.classification_report(data=val_ds)\n",
    "mobilenet_experiment.plot_confusion_matrix(data=val_ds)\n",
    "mobilenet_experiment.evaluate(data=test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Vgg model\n",
    "vgg = factory.create_vgg()\n",
    "vgg.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = vgg.alias\n",
    "vgg_experiment = Experiment(model=vgg, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "vgg_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "vgg_experiment.classification_report(data=val_ds)\n",
    "vgg_experiment.plot_confusion_matrix(data=val_ds)\n",
    "vgg_experiment.evaluate(data=test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Xception model\n",
    "xception = factory.create_xception()\n",
    "xception.summary()\n",
    "\n",
    "# Create and run the experiment.\n",
    "config[\"model\"] = xception.alias\n",
    "xception_experiment = Experiment(model=xception, config=config, repo=repo, optimizer=optimizer, callbacks=callbacks, metrics=metrics, force=force)\n",
    "xception_experiment.run(train_ds=train_ds, val_ds=val_ds)\n",
    "xception_experiment.classification_report(data=val_ds)\n",
    "xception_experiment.plot_confusion_matrix(data=val_ds)\n",
    "xception_experiment.evaluate(data=test_ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
