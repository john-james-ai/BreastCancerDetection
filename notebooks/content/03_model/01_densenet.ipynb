{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "\n",
    "Dense Convolutional Network (DenseNet) {cite}`huangDenselyConnectedConvolutional2018a`, introduced by Gao Huang and Kilian Weinberger of Cornell University,  Zhuang Liu of Tsinghua University, and Laurens van der Maaten of Facebook AI Research (FAIR) is among the most influential convolutional neural network (CNN) architectures of the past decade. With over 41,600 citations, the publication received Best Paper honors in the 2017 Conference on Computer Vision and Pattern Recognition (CVPR). \n",
    "\n",
    "## DenseNet Architecture\n",
    "A typical convolutional neural network (CNN) architecture is comprised of one or more blocks of convolutional and pooling layers, fully connected layer(s), and an output layer that generates the predictions. \n",
    "\n",
    "```{figure} ../../figures/cnn.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Traditional Convolutional Network\n",
    "```\n",
    "\n",
    "Each layer, generally speaking, takes feature maps from the previous layer, performs a convolution or pooling operation, and produces feature maps for the subsequent layer. For the $L$ layers in a CNN, there are typically $L$ connections – one between each layer and its subsequent layer.\n",
    "\n",
    "As CNNs become deeper, and the number of layers increases, the problem of vanishing gradients becomes emergent. Vanishing gradients occur when the partial derivatives of the activation functions gradually diminish as they propagate back through the network.\n",
    "\n",
    "In a DenseNet architecture, each layer is connected to every other layer, thereby increasing information flow through the network, strengthening feature propagation, and alleviating the vanishing gradient problem. \n",
    "\n",
    "Each layer $l$ in a DenseNet architecture receives $l$ inputs, consisting of concatenated feature maps from all preceding layers, and passes its feature maps to all $L-l$ subsequent layers. {numref}`densenet_blocks` illustrates the layout. \n",
    "\n",
    "```{figure} ../../figures/densenet_architecture.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Densely Connected Convolutional Network\n",
    "```\n",
    "\n",
    "Hence a $L$ layer DenseNet has $\\frac{L(L+1)}{2}$ connections. \n",
    "\n",
    "```{math}\n",
    ":label: densenet_input\n",
    "X_l=\\mathcal{H}_l([x_0,x_1,…,x_{l-1}),\n",
    "```\n",
    "\n",
    "where $[x_0,x_1,…,x_{l-1}]$ refers to the concatenation of the feature maps produced in layers $0,…,l-1$.\n",
    "\n",
    "The DenseNet architecture can be further described in terms of its dense blocks, transition layers, bottleneck layers, and growth rate.\n",
    "\n",
    "### Dense Blocks\n",
    "\n",
    "A DenseNet is comprised of four (4) dense blocks, each containing four composite functions. Each composite function performs three consecutive operations: batch normalization (BN), followed by a rectified linear unit (ReLU), then a 3 x 3 convolution (Conv) as shown in {numref}`densenet_dense_block`.\n",
    "\n",
    "```{figure} ../../figures/densenet_composition.gif\n",
    "---\n",
    "align: center\n",
    "---\n",
    "DenseNet Composition Layer\n",
    "```\n",
    "\n",
    "Four sets of $k$ channel feature maps are generated, concatenated, and then passed on to a transition layer.\n",
    "\n",
    "### Transition Layer\n",
    "\n",
    "Between each dense block, transition layers perform the convolution and pooling operation, down-sampling the feature maps for subsequent dense blocks. \n",
    "\n",
    "```{figure} ../../figures/dense_blocks.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Multiple Dense Blocks and Transition Layers\n",
    "```\n",
    "\n",
    "The transition layer consists of a batch normalization layer, a 1 x 1 convolutional layer, and a 2 x 2 average pooling layer.\n",
    "\n",
    "### Bottleneck Layers\n",
    "\n",
    "Bottleneck layers reduce the number of feature maps in the dense blocks and improve computational efficiency. A 1 x 1 convolution reduces the output channel dimensions before the 3 x 3 convolution in each block.\n",
    "\n",
    "```{figure} ../../figures/densenet_bottleneck.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Bottleneck Layers\n",
    "```\n",
    "\n",
    "The BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3X3) above is referred to as the DenseNet-B Bottleneck Layer.\n",
    "\n",
    "### Growth Rate\n",
    "\n",
    "DenseNet layers are also very narrow, in that each layer contributes $k$ feature maps to the ‘collective knowledge’ of the network, while the remaining feature maps are kept unchanged. \n",
    "```{figure} ../../figures/densenet_growth.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "DenseNet Growth Rate\n",
    "```\n",
    "The hyperparameter $k$ is known as the *growth rate* of the network. It regulates how much new information each layer contributes to the global state. Once written, the global state can be accessed everywhere in the network; hence, no need to replicate it from layer to layer.\n",
    "\n",
    "## DenseNet Architecture Benefits \n",
    "Several key benefits are derived from the DenseNet architecture.\n",
    "\n",
    "### Parameter Efficiency: \n",
    "DenseNet can achieve SOTA performance matching that of ResNet with only 0.8M parameters, as compared to 10.2M parameters with ResNet. DenseNet250 with 15.3M parameters consistently outperforms models such as FractaNet and Wide ResNets that have more than 30M parameters.\n",
    "\n",
    "### Computational Efficiency\n",
    "```{figure} ../../figures/densenet_efficiency.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "DenseNet Computational Efficiency\n",
    "```\n",
    "The number of parameters in the ResNet model is directly proportional to the number of input and output feature maps:  C x C; whereas, the number of parameters in the DenseNet architecture is directly proportional to the number of layers and the growth rate as ($l$ x $k$ x $k$), where $k<<C$.\n",
    "\n",
    "### Improved Gradient Flow\n",
    "Each layer has direct access to the gradients from the loss function and to the original input signal, which helps with the training of deeper network architectures.\n",
    "\n",
    "### Reduced Overfitting\n",
    "Dense connections have a regularizing effect which reduces overfitting on tasks with smaller training sets.\n",
    "\n",
    "### Implicit Deep Supervision:\n",
    "A single DenseNet loss function performs deep supervision of all layers through at most two or three transition layers. \n",
    "\n",
    "```{figure} ../../figures/densenet_deep_supervision.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Densely Connected Convolutional Network\n",
    "```\n",
    "\n",
    "### Diversified Features\n",
    "Since each layer in DenseNet receives all the output from the preceding layers, it is exposed to a more diversified feature set which tends to present richer patterns. \n",
    "\n",
    "```{figure} ../../figures/densenet_diversified.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Densely Connected Convolutional Network\n",
    "```\n",
    "\n",
    "### Low Complexity Features\n",
    "In standard convolutional networks, classifiers make predictions based on the high complexity features of the top of the network. \n",
    "\n",
    "```{figure} ../../figures/densenet_std_feature_complexity.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "Traditional Convolutional Networks High Complexity Features\n",
    "```\n",
    "As a consequence of the dense connection architecture, DenseNet classifiers leverage the range of feature complexity levels when making predictions. \n",
    "\n",
    "```{figure} ../../figures/densenet_feature_complexity.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "DenseNet Full Range of Feature Complexity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (/home/john/anaconda3/envs/bcd/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/experimental/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRepo\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseNetFactory\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransfer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FineTuner\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Historian, LRLogger\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearThawSchedule, CosineDecayLearningRateFactory, BCDCosineDecay\n",
      "File \u001b[0;32m~/projects/bcd/bcd/model/transfer.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Historian, LRLogger\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRepo\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningRateScheduleFactory, ThawSchedule\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------------------ #\u001b[39;00m\n\u001b[1;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(stream\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout)\n",
      "File \u001b[0;32m~/projects/bcd/bcd/model/schedule.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------------------ #\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------------------ #\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#                                   THAW SCHEDULE                                                  #\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------------------ #\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mThawSchedule\u001b[39;00m(ABC):\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/__internal__/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/__internal__/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/src/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the TF-Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/src/applications/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/src/applications/convnext.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/src/backend.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/tf_keras/src/dtensor/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras' DTensor library.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor \u001b[38;5;28;01mas\u001b[39;00m dtensor_api\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (/home/john/anaconda3/envs/bcd/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v2/experimental/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pathlib\n",
    "from glob import glob\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow as tf\n",
    "from bcd.model.repo import ModelRepo\n",
    "from bcd.model.factory import DenseNetFactory\n",
    "from bcd.model.transfer import FineTuner\n",
    "from bcd.model.callback import Historian, LRLogger\n",
    "from bcd.model.schedule import LinearThawSchedule, CosineDecayLearningRateFactory, BCDCosineDecay\n",
    "pd.set_option('display.max_rows',999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"01_densenet.ipynb\"\n",
    "project = \"breast_cancer_detection\"\n",
    "\n",
    "# Model Parameters\n",
    "full_dataset = False\n",
    "model_name = \"densenet\" if full_dataset else \"densenet_10\"\n",
    "version=\"v2\"\n",
    "name = f\"{model_name}_{version}\"\n",
    "metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "force = False  # Whether to retrain if the model and weights already exist from a prior training session.\n",
    "base_model_layer = 5 # Layer of the DenseNet base model\n",
    "loss = \"binary_crossentropy\"\n",
    "activation = \"sigmoid\"\n",
    "\n",
    "# Dataset params\n",
    "batch_size = 64 if full_dataset else 32\n",
    "input_shape = (224,224,3)\n",
    "output_shape = 1\n",
    "train_dir = pathlib.Path(\"data/image/1_final/training/training/\").with_suffix('') if full_dataset else pathlib.Path(\"data/image/1_final/training_10/training/\").with_suffix('') \n",
    "test_dir = pathlib.Path(\"data/image/1_final/test/test/\").with_suffix('')\n",
    "\n",
    "# Feature Extraction Parameters\n",
    "initial_epochs = 100  # Number of epochs to train for feature extraction\n",
    "\n",
    "# Fine Tune Parameters\n",
    "fine_tune_epochs = 20  # Number of epochs for each fine tune session\n",
    "fine_tune_sessions = 10 # Number of fine tune sessions\n",
    "fine_tune_min_lr = 1e-10\n",
    "fine_tune_max_lr = 1e-5\n",
    "\n",
    "\n",
    "# Early stop parameters \n",
    "es_min_delta = 0.0001\n",
    "es_monitor = \"val_loss\"  # Monitor validation loss for early stopping\n",
    "es_patience = 10  # The number of consecutive epochs for which lack of improvement is tolerated \n",
    "es_restore_best_weights = True  # Returns the best weights rather than the weights at the last epoch.\n",
    "es_verbose = 1\n",
    "es_session_patience = None # The number of fine tuning sessions for which lack of improvement is tolerated. If None, no early stopping at session level\n",
    "\n",
    "# Reduce LR on Plateau Parameters\n",
    "rlr_monitor = \"val_loss\"\n",
    "rlr_factor = 0.2\n",
    "rlr_patience = 3\n",
    "rlr_verbose = 1\n",
    "rlr_mode = \"auto\"\n",
    "rlr_min_delta = 0.0001\n",
    "rlr_min_lr=1e-10\n",
    "\n",
    "# ModelCheckpoint Callback parameters\n",
    "mcp_location = \"models/\"\n",
    "mcp_mode = \"auto\"\n",
    "mcp_save_weights_only = False\n",
    "mcp_save_best_only = True\n",
    "mcp_save_freq = \"epoch\"\n",
    "mcp_verbose = 1\n",
    "\n",
    "# Historian parameters\n",
    "filename = f\"{name}_history.pkl\"\n",
    "historian_filepath = os.path.join(\"models\", name, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"architecture\": \"cnn\",\n",
    "    \"model_name\": model_name,\n",
    "    \"version\": version,\n",
    "    \"dataset\": \"CBIS-DDSM-10\",\n",
    "    \"batch_size\": batch_size,\n",
    "    \"initial_epochs\": initial_epochs,\n",
    "    \"fine_tune_epochs\": fine_tune_epochs,\n",
    "    \"fine_tune_sessions\": fine_tune_sessions,\n",
    "    \"fine_tune_min_lr\": fine_tune_min_lr,\n",
    "    \"fine_tune_max_lr\": fine_tune_max_lr,\n",
    "    \"early_stop_min_delta\": es_min_delta,\n",
    "    \"early_stop_monitor\": es_monitor,\n",
    "    \"early_stop_patience\": es_patience,\n",
    "    \"early_stop_restore_best_weights\": es_restore_best_weights,\n",
    "    \"early_stop_verbose\": es_verbose,\n",
    "    \"rlr_monitor\": rlr_monitor,\n",
    "    \"rlr_factor\": rlr_factor,\n",
    "    \"rlr_patience\": rlr_patience,\n",
    "    \"rlr_verbose\": rlr_verbose,\n",
    "    \"rlr_mode\": rlr_mode,\n",
    "    \"rlr_min_delta\": rlr_min_delta,\n",
    "    \"rlr_min_lr\": rlr_min_lr\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=project, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DataSet (10%)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Validation DataSet (10%)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Test Set\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Several dependencies will be used throughout this notebook, including:\n",
    "- Early Stop Callback\n",
    "- Reduce Learning Rate on Plateau Callback\n",
    "- Model Repository\n",
    "- DenseNet Model Factory\n",
    "- Historian Callback\n",
    "\n",
    "We'll make those objects available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=es_monitor, \n",
    "                                                       min_delta=es_min_delta,\n",
    "                                                       patience=es_patience, \n",
    "                                                       restore_best_weights=es_restore_best_weights,\n",
    "                                                       verbose=es_verbose)\n",
    "wandb_callback = wandb.keras.WandbCallback(\n",
    "    monitor=es_monitor, verbose=1, mode=\"auto\", save_model=False\n",
    ")\n",
    "\n",
    "learning_rate_factory = CosineDecayLearningRateFactory(sessions=fine_tune_sessions, \n",
    "                                                       epochs=fine_tune_epochs, \n",
    "                                                       batches=train_ds.cardinality().numpy(), \n",
    "                                                       min_lr=fine_tune_min_lr, \n",
    "                                                       max_lr=fine_tune_max_lr, \n",
    "                                                       warmup_phase=0.5)\n",
    "\n",
    "repo = ModelRepo(location=mcp_location)\n",
    "\n",
    "factory = DenseNetFactory()\n",
    "\n",
    "historian = Historian(name=name)\n",
    "\n",
    "thaw_schedule = LinearThawSchedule(sessions=fine_tune_sessions, \n",
    "                                   base_model_layer=base_model_layer, \n",
    "                                   n_layers=len(factory.base_model.layers))\n",
    "\n",
    "learning_rate_schedule = BCDCosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=(decay_epochs * train_ds.cardinality().numpy()),\n",
    "    alpha=final_learning_rate,\n",
    "    warmup_target=target_learning_rate,\n",
    "    warmup_steps=warmup_epochs * train_ds.cardinality().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "If the model already exists, obtain it from the repository. Otherwise, create the model and perform feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"feature_extraction\"\n",
    "if force or not repo.exists(name=name, stage=stage):\n",
    "    model = factory.create(input_shape=input_shape, \n",
    "                            output_shape=output_shape,\n",
    "                            activation=activation)\n",
    "    # Create the optimizer\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "    # Compile the Model\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,        \n",
    "    )\n",
    "\n",
    "    # Delete existing checkpoints\n",
    "    repo.remove(name=name, stage=stage)\n",
    "    # Summarize the model\n",
    "    model.summary()\n",
    "    # Create Learning Rate Logger Callback\n",
    "    lrlogger = LRLogger(optimizer=optimizer)    \n",
    "    # Obtain a checkpoint callback from the model repository \n",
    "    checkpoint_callback = repo.create_callback(name=name, stage=stage)\n",
    "    # Set the session on the historian to 0 for feature extraction\n",
    "    historian.on_session_begin(session=0)\n",
    "    # Fit the model with callbacks\n",
    "    history = model.fit(train_ds, \n",
    "                        validation_data=val_ds, \n",
    "                        epochs=initial_epochs, \n",
    "                        callbacks=[checkpoint_callback, \n",
    "                                   early_stop_callback, \n",
    "                                   wandb_callback,\n",
    "                                   lrlogger,\n",
    "                                   historian])        \n",
    "    # Register the model on wandb\n",
    "    filepath = repo.get_filepath(name=name, stage=stage)    \n",
    "    artifact = wandb.Artifact(f\"{name}-{stage}-{run.id}\", type=\"model\")\n",
    "    artifact.add_file(filepath)\n",
    "    wandb.log_artifact(artifact, aliases=[stage, \"best\"])\n",
    "    wandb.run.link_artifact(artifact, \"aistudio/breast_cancer_detection/DenseNet\")\n",
    "else:\n",
    "    # Obtain the model from the repository\n",
    "    model = repo.get(name=name, stage=stage)\n",
    "    # Obtain the historian\n",
    "    historian = historian.load(historian_filepath)    \n",
    "    # Summarize the model\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction appears to have converged in fourteen epochs. Our early stopping callback stopped training when validation loss didn't improve in 3 epochs. Training and validation accuracy of 55% and 50% respectively, alludes to the considerable difference between source and target datasets. Gradually fine tuning the network on the CBIS-DDSM dataset will allow the network to adapt to the features and characteristics of our dataset.\n",
    "\n",
    "The next cell will iteratively run a series of 10 fine tuning sessions in which the DenseNet model is gradually unfrozen in 10 increments. Each increment increases the number of layers to be thawed logarithmically from 1 layer to 707 layers of the DenseNet model. Learning rates will be decayed logarithmically from 1e-4 to 1e-10 to mitigate catastrophic forgetting within the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. We can now move on to the fine tuning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ft = FineTuner(name=name, \n",
    "               train_ds=train_ds, \n",
    "               validation_ds=val_ds, \n",
    "               repo=repo,\n",
    "               thaw_schedule=thaw_schedule,\n",
    "               learning_rate_schedule_factory=learning_rate_factory,\n",
    "               metrics=metrics,\n",
    "               fine_tune_epochs=fine_tune_epochs,\n",
    "               sessions=fine_tune_sessions,\n",
    "               callbacks=[early_stop_callback, wandb_callback],\n",
    "               )\n",
    "ft.tune(model=model, historian=historian, force=True)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning has completed. Let's check the learning curves."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot the historian\n",
    "historian.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's evaluate generalization performance on the test set. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = model.evaluate(test_ds, batch_size=batch_size)\n",
    "print(\"Test Loss, Test Accuracy: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
