{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The prior data quality analysis revealed several data anomalies requiring attention. \n",
    "\n",
    "1. Cases with invalid values for subtlety and breast density.\n",
    "2. Missing calcification type, calcification distribution, mass shape, and mass margins data. \n",
    "3. Categories that have different spelling, but the same meaning. \n",
    "\n",
    "In addition, processing and analyzing calcification and mass case datasets separately doesn't support a comprehensive, top-down view of the data. The following tasks will eliminate redundancy and produce a clean dataset for downstream analyses.\n",
    "\n",
    "| # | Task                                                                             |\n",
    "|---|----------------------------------------------------------------------------------|\n",
    "| 1 | Create a combined calcification and mass case dataset for analysis.              |\n",
    "| 2 | Replace invalid values for breast density with NA for imputation.                |\n",
    "| 3 | Set mass and calcification morphological values to NA where they do not   apply. |\n",
    "| 4 | Remove cases with invalid values for subtlety                                    |\n",
    "| 5 | Replace calcification types 'LUCENT_CENTER' with 'LUCENT_CENTERED', and 'PLEOMORPHIC-PLEOMORPHIC', with 'PLEOMORPHIC' |\n",
    "| 6 | Impute missing values using Multiple Imputation by Chained Equations   (MICE)    |\n",
    "| 7 | Convert data to the appropriate data types                                       |\n",
    "| 8 | Conduct random inspection of imputations.                                        |\n",
    "| 9 | Conduct final data quality analysis.                                             |\n",
    "\n",
    "Once the case dataset has been cleaned, we will merge it with the DICOM dataset to create the DICOM-CASE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'jbook' in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bcd.data.prep.meta.case import CaseImputer\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "from bcd.data.quality.case import MassCaseDQA, CalcCaseDQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_STAGED_CALC = \"data/staged/calc_cases.csv\"\n",
    "FP_STAGED_MASS = \"data/staged/mass_cases.csv\"\n",
    "FP_STAGED = \"data/staged/cases.csv\"\n",
    "\n",
    "FP_CLEAN_CASES = \"data/clean/cases.csv\"\n",
    "FP_CLEAN_CALC =  \"data/clean/calc_cases.csv\"\n",
    "FP_CLEAN_MASS =  \"data/clean/mass_cases.csv\"\n",
    "CASES_TO_REMOVE = ['P_00710_RIGHT_mass_MLO_1', 'P_00710_RIGHT_mass_CC_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Combined Case Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.read_csv(FP_STAGED_CALC)\n",
    "dfm = pd.read_csv(FP_STAGED_MASS)\n",
    "df = pd.concat([dfc,dfm], axis=0)\n",
    "df_orig = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Density\n",
    "Replace invalid values for breast density with NA for downstream imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set invalid values for breast_density to NA\n",
    "df['breast_density'] = df['breast_density'].replace(0, np.NAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphology Not Applicable\n",
    "Set morphological variables to NA where they do not apply. For instance mass shape doesn't apply to calcification cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set case and mass data to NOT APPLICABLE where appropriate.\n",
    "df.loc[df['abnormality_type'] == 'calcification', 'mass_shape'] = 'NOT APPLICABLE'\n",
    "df.loc[df['abnormality_type'] == 'calcification', 'mass_margins'] = 'NOT APPLICABLE'\n",
    "df.loc[df['abnormality_type'] == 'mass', 'calc_type'] = 'NOT APPLICABLE'\n",
    "df.loc[df['abnormality_type'] == 'mass', 'calc_distribution'] = 'NOT APPLICABLE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Removal\n",
    "Remove cases with invalid values for subtlety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/john/projects/bcd/notebooks/content/04_clean.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[\u001b[39m~\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mcase_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(CASES_TO_REMOVE)]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA total of \u001b[39m\u001b[39m{\u001b[39;00mdf_orig[\u001b[39m0\u001b[39;49m]\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mdf\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m rows were removed from the dataset, leaving \u001b[39m\u001b[39m{\u001b[39;00mdf\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m rows.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(msg)\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/bcd/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "df = df.loc[~df['case_id'].isin(CASES_TO_REMOVE)]\n",
    "msg = f\"A total of {df_orig.shape[0] - df.shape[0]} rows were removed from the dataset, leaving {df.shape[0]} rows.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Alignment\n",
    "Align categories that have the same meaning, but differ in spelling only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['calc_type'] == 'LUCENT_CENTER', 'calc_type'] = 'LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'ROUND_AND_REGULAR-LUCENT_CENTER-DYSTROPHIC', 'calc_type'] = 'ROUND_AND_REGULAR-LUCENT_CENTERED-DYSTROPHIC'\n",
    "df.loc[df['calc_type'] == 'PUNCTATE-LUCENT_CENTER', 'calc_type'] = 'PUNCTATE-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'VASCULAR-COARSE-LUCENT_CENTER-ROUND_AND_REGULAR-PUNCTATE', 'calc_type'] = 'VASCULAR-COARSE-LUCENT_CENTERED-ROUND_AND_REGULAR-PUNCTATE'\n",
    "df.loc[df['calc_type'] == 'ROUND_AND_REGULAR-LUCENT_CENTER', 'calc_type'] = 'ROUND_AND_REGULAR-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'LUCENT_CENTER-PUNCTATE', 'calc_type'] = 'LUCENT_CENTERED-PUNCTATE'\n",
    "df.loc[df['calc_type'] == 'COARSE-ROUND_AND_REGULAR-LUCENT_CENTER', 'calc_type'] = 'COARSE-ROUND_AND_REGULAR-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'ROUND_AND_REGULAR-LUCENT_CENTER-PUNCTATE', 'calc_type'] = 'ROUND_AND_REGULAR-LUCENT_CENTERED-PUNCTATE'\n",
    "df.loc[df['calc_type'] == 'COARSE-LUCENT_CENTER', 'calc_type'] = 'COARSE-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'PLEOMORPHIC-PLEOMORPHIC', 'calc_type'] = 'PLEOMORPHIC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "Multiple Imputation by Chained Equations (MICE) is a robust, informative method of estimating missing values in datasets. The procedure imputes missing data through an iterative series of predictive models which estimate the value of missing data using the other variables in the dataset. For this, we'll use our CaseImputer which wraps scikit-learn's IterativeImputer implementation of MICE.\n",
    "\n",
    "First, let's capture the missing values as we will inspect them after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/john/projects/bcd/notebooks/content/04_clean.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m null_mask \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39many(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df_missing \u001b[39m=\u001b[39m df[null_mask]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00mdf_missing\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m rows (approximately \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(df_missing\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mdf_orig\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m% of the dataset) with missing data in the dataset.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/john/projects/bcd/notebooks/content/04_clean.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(msg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# Grab rows with missing data\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "df_missing = df[null_mask]\n",
    "msg = f\"There are {df_missing.shape[0]} rows (approximately {round(df_missing.shape[0] / df_orig.shape[0] * 100,1)}% of the dataset) with missing data in the dataset.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = CaseImputer(random_state=5)\n",
    "imp.fit(df=df)\n",
    "df_clean = imp.transform(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let's save the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"patient_id\": \"object\",\n",
    "\"breast_density\": \"int32\",\n",
    "\"left_or_right_breast\": \"category\",\n",
    "\"image_view\": \"category\",\n",
    "\"abnormality_id\": \"int32\",\n",
    "\"abnormality_type\": \"category\",\n",
    "\"calc_type\": \"category\",\n",
    "\"calc_distribution\": \"category\",\n",
    "\"assessment\": \"int32\",\n",
    "\"pathology\": \"category\",\n",
    "\"subtlety\": \"int32\",\n",
    "\"fileset\": \"object\",\n",
    "\"mass_shape\": \"category\",\n",
    "\"mass_margins\": \"category\",\n",
    "\"case_id\": \"object\",\n",
    "\"cancer\": \"bool\",\n",
    "}\n",
    "dtypes_ordinal = {\n",
    "\"breast_density\": \"category\",\n",
    "\"assessment\": \"category\",\n",
    "\"subtlety\": \"category\",\n",
    "}\n",
    "df_clean = df_clean.astype(dtypes)\n",
    "df_clean = df_clean.astype(dtypes_ordinal)\n",
    "df_clean.info()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(FP_CLEAN_CASES), exist_ok=True)\n",
    "df_clean.to_csv(FP_CLEAN_CASES, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a random sampling of the missing data and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cases = df_missing['case_id'].sample(10)\n",
    "df_missing.loc[df_missing['case_id'].isin(sample_cases)]\n",
    "df_clean.loc[df_clean['case_id'].isin(sample_cases)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis 2.0\n",
    "Ok, let's have another go at the data quality analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqc = CalcCaseDQA(filepath=FP_CLEAN_CALC)\n",
    "dqm = MassCaseDQA(filepath=FP_CLEAN_MASS)\n",
    "dqcc = dqc.analyze_completeness()\n",
    "dqmc = dqm.analyze_completeness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dqcc.summary)\n",
    "print(dqmc.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqcu = dqc.analyze_uniqueness()\n",
    "dqmu = dqm.analyze_uniqueness()\n",
    "print(dqcu.summary)\n",
    "print(dqmu.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqcv = dqc.analyze_validity()\n",
    "dqmv = dqm.analyze_validity()\n",
    "print(dqcv.summary)\n",
    "print(dqmv.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're valid. This concludes the data cleaning exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
