{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'learning_curve' from 'bcd.model.visual' (/home/john/projects/bcd/bcd/model/visual.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseNetFactory\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m learning_curve\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbcd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thaw\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'learning_curve' from 'bcd.model.visual' (/home/john/projects/bcd/bcd/model/visual.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "from bcd.model.factory import DenseNetFactory\n",
    "from bcd.model.visual import X4LearningVisualizer\n",
    "from bcd.utils.model import thaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_shape = (224,224,3)\n",
    "output_shape = 1\n",
    "\n",
    "initial_epochs = 100  # Number of epochs to train for feature extraction\n",
    "\n",
    "fine_tune_epochs = 10  # Number of epochs for each fine tune round\n",
    "fine_tune_thaw_rate = 0.05  # Additional proportion of layers to unthaw each fine tune round. \n",
    "\n",
    "early_stop_monitor = \"val_loss\"  # Monitor validation loss for early stopping\n",
    "early_stop_patience = 3  # The number of consecutive epochs for which lack of improvement is tolerated \n",
    "early_stop_restore_best_weights = True  # Returns the best weights rather than the weights at the last epoch.\n",
    "\n",
    "learning_rate_base = 0.0001  # Base learning rate for the Adam optimizer \n",
    "learning_rate_decay = 10 # The factor by which learning rate decays for each fine tune round.\n",
    "\n",
    "loss = \"binary_crossentropy\"\n",
    "activation = \"sigmoid\"\n",
    "metric = \"accuracy\"\n",
    "\n",
    "train_dir_10 = pathlib.Path(\"data/image/1_final/training_10/training/\").with_suffix('')\n",
    "test_dir = pathlib.Path(\"data/image/1_final/test/test/\").with_suffix('')\n",
    "checkpoint_path = \"models/checkpoints/densenet.ckpt\"\n",
    "model_directory = \"models/densenet/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries - Callbacks\n",
    "Create a callback checkpoint that will automatically save model weights at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True, # set to False to save the entire model\n",
    "                                                         save_best_only=True, # save only the best model weights instead of a model every epoch\n",
    "                                                         save_freq=\"epoch\", # save every epoch\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an early stopping callback that will stop training if validation loss doesn't improve in n epochs\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=early_stop_monitor, patience=early_stop_patience, restore_best_weights=early_stop_restore_best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DataSet (10%)\n",
    "train_ds_10 = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir_10,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Validation DataSet (10%)\n",
    "val_ds_10 = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir_10,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Test Set\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = DenseNetFactory()\n",
    "densenet = factory.create(input_shape=input_shape, output_shape=output_shape, learning_rate=learning_rate_base, trainable=False, loss=loss, activation=activation, metric=metric)\n",
    "densenet_history = densenet.fit(train_ds_10, epochs=initial_epochs, validation_data=val_ds_10, callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_feature_extraction.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4v = X4LearningVisualizer(name=\"densenet\")\n",
    "x4v(history=densenet_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an training accuracy of 60% and validation accuracy of 70% on 10% of the data in just 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tune Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_round = 1\n",
    "densenet_ft_epochs = initial_epochs + (fine_tune_epochs * fine_tune_round)\n",
    "\n",
    "# Determine the number of layers to thaw.\n",
    "densenet_base_layer = 5\n",
    "densenet_base = densenet.layers[densenet_base_layer]\n",
    "densenet_layers = densenet_base.layers\n",
    "densenet_n_layers = len(densenet_layers)\n",
    "thaw_n_layers = int(densenet_n_layers * fine_tune_thaw_rate) * fine_tune_round\n",
    "\n",
    "# Freeze all but the top thaw_n_layers\n",
    "densenet = thaw(n=thaw_n_layers, model=densenet, base_model_layer=densenet_base_layer)\n",
    "\n",
    "# Recompile the model\n",
    "densenet.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=(learning_rate_base/(learning_rate_decay**fine_tune_round))), metrics=[metric])\n",
    "\n",
    "# Check trainable weights in summary\n",
    "densenet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densenet_history_ft_1 = densenet.fit(train_ds_10, epochs=densenet_ft_epochs, validation_data=val_ds_10, \n",
    "                                         initial_epoch=densenet_history.epoch[-1],\n",
    "                                         callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_fine_tuning_1_1.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tune Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_round = 2\n",
    "densenet_ft_epochs = initial_epochs + (fine_tune_epochs * fine_tune_round)\n",
    "\n",
    "# Determine the number of layers to thaw.\n",
    "thaw_n_layers = thaw_n_layers * fine_tune_round\n",
    "\n",
    "# Freeze all but the top thaw_n_layers\n",
    "densenet = thaw(n=thaw_n_layers, model=densenet, base_model_layer=densenet_base_layer)\n",
    "\n",
    "# Recompile the model\n",
    "densenet.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=(learning_rate_base/(learning_rate_decay**fine_tune_round))), metrics=[metric])\n",
    "\n",
    "# Check trainable weights in summary\n",
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densenet_history_ft_2 = densenet.fit(train_ds_10, epochs=densenet_ft_epochs, validation_data=val_ds_10, \n",
    "                                         initial_epoch=densenet_history_ft_1.epoch[-1],\n",
    "                                         callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_fine_tuning_1_2.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tune Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_round = 3\n",
    "densenet_ft_epochs = initial_epochs + (fine_tune_epochs * fine_tune_round)\n",
    "\n",
    "# Determine the number of layers to thaw.\n",
    "thaw_n_layers = thaw_n_layers * fine_tune_round\n",
    "\n",
    "# Freeze all but the top thaw_n_layers\n",
    "densenet = thaw(n=thaw_n_layers, model=densenet, base_model_layer=densenet_base_layer)\n",
    "\n",
    "# Recompile the model\n",
    "densenet.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=(learning_rate_base/(learning_rate_decay**fine_tune_round))), metrics=[metric])\n",
    "\n",
    "# Check trainable weights in summary\n",
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densenet_history_ft_3 = densenet.fit(train_ds_10, epochs=densenet_ft_epochs, validation_data=val_ds_10, \n",
    "                                         initial_epoch=densenet_history_ft_2.epoch[-1],\n",
    "                                         callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_fine_tuning_1_3.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Weights from Feature Extraction Stage\n",
    "filename = \"densenet_feature_extraction.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet = tf.keras.models.load_model(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_round = 1\n",
    "densenet_ft_epochs = initial_epochs + (fine_tune_epochs * fine_tune_round)\n",
    "\n",
    "# Determine the number of layers to thaw.\n",
    "thaw_n_layers = thaw_n_layers * fine_tune_round\n",
    "\n",
    "# Freeze all but the top thaw_n_layers\n",
    "densenet = thaw(n=thaw_n_layers, model=densenet, base_model_layer=densenet_base_layer)\n",
    "\n",
    "# Recompile the model\n",
    "densenet.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=(learning_rate_base/(learning_rate_decay**fine_tune_round))), metrics=[metric])\n",
    "\n",
    "# Check trainable weights in summary\n",
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densenet_history_ft_2_1 = densenet.fit(train_ds_10, epochs=densenet_ft_epochs, validation_data=val_ds_10, \n",
    "                                         initial_epoch=densenet_history.epoch[-1],\n",
    "                                         callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_fine_tuning_2_1.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_round = 2\n",
    "densenet_ft_epochs = initial_epochs + (fine_tune_epochs * fine_tune_round)\n",
    "\n",
    "# Determine the number of layers to thaw.\n",
    "thaw_n_layers = thaw_n_layers * fine_tune_round\n",
    "\n",
    "# Freeze all but the top thaw_n_layers\n",
    "densenet = thaw(n=thaw_n_layers, model=densenet, base_model_layer=densenet_base_layer)\n",
    "\n",
    "# Recompile the model\n",
    "densenet.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=(learning_rate_base/(learning_rate_decay**fine_tune_round))), metrics=[metric])\n",
    "\n",
    "# Check trainable weights in summary\n",
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densenet_history_ft_2_2 = densenet.fit(train_ds_10, epochs=densenet_ft_epochs, validation_data=val_ds_10, \n",
    "                                         initial_epoch=densenet_history_ft_2_1.epoch[-1],\n",
    "                                         callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_fine_tuning_2_2.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_round = 3\n",
    "densenet_ft_epochs = initial_epochs + (fine_tune_epochs * fine_tune_round)\n",
    "\n",
    "# Determine the number of layers to thaw.\n",
    "thaw_n_layers = thaw_n_layers * fine_tune_round\n",
    "\n",
    "# Freeze all but the top thaw_n_layers\n",
    "densenet = thaw(n=thaw_n_layers, model=densenet, base_model_layer=densenet_base_layer)\n",
    "\n",
    "# Recompile the model\n",
    "densenet.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=(learning_rate_base/(learning_rate_decay**fine_tune_round))), metrics=[metric])\n",
    "\n",
    "# Check trainable weights in summary\n",
    "densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "densenet_history_ft_2_3 = densenet.fit(train_ds_10, epochs=densenet_ft_epochs, validation_data=val_ds_10, \n",
    "                                         initial_epoch=densenet_history_ft_2_2.epoch[-1],\n",
    "                                         callbacks=[checkpoint_callback, early_stop_callback])\n",
    "filename = \"densenet_fine_tuning_2_3.keras\"\n",
    "filepath = os.path.join(model_directory, filename)\n",
    "densenet.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_history_ft_2_3.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
