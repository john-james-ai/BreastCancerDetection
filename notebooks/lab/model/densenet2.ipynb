{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet \n",
    "Todo: Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from glob import glob\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "from bcd.model.repo import ModelRepo\n",
    "from bcd.model.factory import DenseNetFactory\n",
    "from bcd.model.transfer import FineTuner\n",
    "from bcd.model.callback import Historian\n",
    "pd.set_option('display.max_rows',999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "batch_size = 32\n",
    "input_shape = (224,224,3)\n",
    "output_shape = 1\n",
    "train_dir = pathlib.Path(\"data/image/1_final/training/training/\").with_suffix('')\n",
    "test_dir = pathlib.Path(\"data/image/1_final/test/test/\").with_suffix('')\n",
    "\n",
    "# Training Params\n",
    "initial_epochs = 100  # Number of epochs to train for feature extraction\n",
    "fine_tune_epochs = 50  # Number of epochs for each fine tune session\n",
    "initial_learning_rate = 0.0001  # Base learning rate for the Adam optimizer \n",
    "loss = \"binary_crossentropy\"\n",
    "activation = \"sigmoid\"\n",
    "\n",
    "# Early stop callback \n",
    "min_delta = 0.0001\n",
    "monitor = \"val_loss\"  # Monitor validation loss for early stopping\n",
    "patience = 3  # The number of consecutive epochs for which lack of improvement is tolerated \n",
    "restore_best_weights = True  # Returns the best weights rather than the weights at the last epoch.\n",
    "\n",
    "\n",
    "# ModelCheckpoint Callback parameters\n",
    "location = \"models/\"\n",
    "mode = \"auto\"\n",
    "save_weights_only = False\n",
    "save_best_only = True\n",
    "save_freq = \"epoch\"\n",
    "verbose = 1\n",
    "\n",
    "# Historian parameters\n",
    "historian_filepath = \"models/densenet/densenet_history.pkl\"\n",
    "\n",
    "# Model Parameters\n",
    "name = \"densenet\"\n",
    "metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "model_factory = DenseNetFactory\n",
    "force = False  # Whether to retrain if the model and weights already exist from a prior training session.\n",
    "base_model_layer = 5 # Layer of the DenseNet base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Several dependencies will be used throughout this notebook, including:\n",
    "- Early Stop Callback\n",
    "- Learning Rate Warmup\n",
    "- Model Repository\n",
    "- DenseNet Model Factory\n",
    "- Historian Callback\n",
    "\n",
    "We'll make those objects available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=monitor, \n",
    "                                                       min_delta=min_delta,\n",
    "                                                       patience=patience, \n",
    "                                                       restore_best_weights=restore_best_weights)\n",
    "learning_rate_warmup = tfm.optimization.LinearWarmup(after_warmup_lr_sched=0.0001,\n",
    "                                                     warmup_steps=1000,\n",
    "                                                     warmup_learning_rate=0.00001)\n",
    "repo = ModelRepo(location=location)\n",
    "factory = model_factory()\n",
    "historian = Historian(name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DataSet (10%)\n",
    "train_ds_10 = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Validation DataSet (10%)\n",
    "val_ds_10 = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation=\"bilinear\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Test Set\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(224,224),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "If the model already exists, obtain it from the repository. Otherwise, create the model and perform feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"feature_extraction\"\n",
    "if force or not repo.exists(name=name, stage=stage):\n",
    "    model = factory.create(input_shape=input_shape, \n",
    "                            output_shape=output_shape, \n",
    "                            learning_rate=initial_learning_rate, \n",
    "                            trainable=False, \n",
    "                            loss=loss, \n",
    "                            activation=activation, \n",
    "                            metrics=metrics)\n",
    "    # Delete existing checkpoints\n",
    "    repo.remove(name=name, stage=stage)\n",
    "    # Summarize the model\n",
    "    model.summary()\n",
    "    # Obtain a checkpoint callback from the model repository \n",
    "    checkpoint_callback = repo.create_callback(name=name, stage=stage)\n",
    "    # Set the session on the historian to 0 for feature extraction\n",
    "    historian.on_session_begin(session=0)\n",
    "    # Fit the model with callbacks\n",
    "    history = model.fit(train_ds_10, epochs=initial_epochs, validation_data=val_ds_10, callbacks=[checkpoint_callback, early_stop_callback, historian])        \n",
    "    \n",
    "else:\n",
    "    # Obtain the model from the repository\n",
    "    model = repo.get(name=name, stage=stage)\n",
    "    # Obtain the historian\n",
    "    historian = historian.load(historian_filepath)    \n",
    "    # Summarize the model\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction appears to have converged in fourteen epochs. Our early stopping callback stopped training when validation loss didn't improve in 3 epochs. Training and validation accuracy of 55% and 50% respectively, alludes to the considerable difference between source and target datasets. Gradually fine tuning the network on the CBIS-DDSM dataset will allow the network to adapt to the features and characteristics of our dataset.\n",
    "\n",
    "The next cell will iteratively run a series of 10 fine tuning sessions in which the DenseNet model is gradually unfrozen in 10 increments. Each increment increases the number of layers to be thawed logarithmically from 1 layer to 707 layers of the DenseNet model. Learning rates will be decayed logarithmically from 1e-4 to 1e-10 to mitigate catastrophic forgetting within the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. We can now move on to the fine tuning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FineTuner(name=name, \n",
    "               train_ds=train_ds_10, \n",
    "               validation_ds=val_ds_10, \n",
    "               repo=repo,\n",
    "               metrics=metrics,\n",
    "               initial_learning_rate=1e-3,\n",
    "               final_learning_rate=1e-6,\n",
    "               callbacks=[early_stop_callback],\n",
    "               )\n",
    "ft.tune(model=model, historian=historian, base_model_layer=base_model_layer, force=force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning has completed. Let's check the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the historian\n",
    "historian.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's evaluate generalization performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds, batch_size=batch_size)\n",
    "print(\"Test Loss, Test Accuracy: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
