{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Range Test\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from bcd.model.callback import LRRangeTestCallback\n",
    "from bcd.model.network.base import NetworkConfig\n",
    "from bcd.model.network.tmnet import TMNetConfig, TMNetFactory\n",
    "from bcd.model.store import ExperimentRepo\n",
    "from bcd.model.pretrained import DenseNet, EfficientNet, Inception, InceptionResNet, MobileNet, ResNet, Xception\n",
    "from bcd.model.experiment import FeatureExtractionExperiment\n",
    "from bcd.model.config import ProjectConfig, DatasetConfig, CheckPointConfig, TrainConfig, EarlyStopConfig, LearningRateScheduleConfig, Config, ExperimentConfig\n",
    "from bcd.model.adapter import LocalAdapter, Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"Development\"\n",
    "force = True\n",
    "base_model = DenseNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform \n",
    "The platform object encapsulates variables that are platform-dependent, such as device type, distribute strategy, api keys, file paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maistudio\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/john/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = LocalAdapter(mode=mode)\n",
    "print(f\"Running on {adapter.device_type}\")\n",
    "\n",
    "# Obtain the TensorFlow state and compute distribution policy, i.e. strategy\n",
    "strategy  = adapter.get_strategy()\n",
    "\n",
    "# Weights and Biases login for model and metric tracking.\n",
    "wandb.login(key=adapter.wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything():\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(adapter: Adapter, mode: str, strategy: tf.distribute.Strategy) -> Config:\n",
    "    \"\"\"Constructs an experiment Config object \"\"\"\n",
    "    # Encapsulates the parameters that define the project in Weights & Biases\n",
    "    project_config = ProjectConfig(\n",
    "        name=\"Breast-Cancer-Detection-Development-Learning-Rate-Range-Test\",\n",
    "        mode=mode)\n",
    "\n",
    "    # The TMNet architecture has two dense layers before a sigmoid activation. We'll set the number of nodes in the last two dense layers to 1024 and 512 respectively. \n",
    "    network_config = TMNetConfig(\n",
    "        activation=\"sigmoid\", \n",
    "        input_shape=(224,224,3), \n",
    "        output_shape=1, \n",
    "        dense1=1048, \n",
    "        dense2=1024)\n",
    "\n",
    "    # The default batch size is 64; however, if running on TPU, the rule of thumb is to optimally set the batch size to 128 * the number of TPU cores    \n",
    "    batch_size = 32 if not adapter.device_type == \"TPU\" else 16 * strategy.num_replicas_in_sync\n",
    "    dataset_config = DatasetConfig(\n",
    "        mode=mode,        \n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # If running on TPU, the learning rate is scaled by the number of cores corresponding to the batch size.\n",
    "    learning_rate = 1e-4 if adapter.device_type != \"TPU\" else 1e-4 *  strategy.num_replicas_in_sync\n",
    "    train_config = TrainConfig(\n",
    "        epochs=10, \n",
    "        learning_rate=learning_rate)    \n",
    "\n",
    "    # Checkpoints will be stored in the directory given by the adapter object. \n",
    "    checkpoint_config = CheckPointConfig(\n",
    "        directory=adapter.model_dir, \n",
    "        monitor=\"val_accuracy\", \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"auto\")\n",
    "\n",
    "    # We'll establish an early stop callback to mitigate overfitting caused by excessive training after validation loss hasn't improved.\n",
    "    early_stop_config = EarlyStopConfig(\n",
    "        min_delta=1e-4, \n",
    "        monitor=\"val_loss\", \n",
    "        patience=10, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1)\n",
    "\n",
    "    # Learning rate will be annealed by a factor of 0.5 if validation loss hasn't improved in 3 epochs.\n",
    "    learning_rate_schedule_config = LearningRateScheduleConfig(        \n",
    "        min_lr=1e-5, \n",
    "        max_lr=1e-1,\n",
    "        epochs=10)\n",
    "    \n",
    "\n",
    "    # The experiment configuration is encapsulated into a single object \n",
    "    return ExperimentConfig(project=project_config, \n",
    "                    dataset=dataset_config, \n",
    "                    train=train_config, \n",
    "                    network=network_config, \n",
    "                    checkpoint=checkpoint_config, \n",
    "                    early_stop=early_stop_config, \n",
    "                    learning_rate_schedule=learning_rate_schedule_config)\n",
    "\n",
    "# Construct the configuration object.\n",
    "config = build_config(adapter=adapter, mode=mode, strategy=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 files belonging to 2 classes.\n",
      "Using 221 files for training.\n",
      "Found 276 files belonging to 2 classes.\n",
      "Using 55 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_dataset(train_dir: str, subset: str, dataset_config: Config, tpu: bool = False) -> tf.data.Dataset:\n",
    "    \"\"\"Produces a TensorFlow training or validation  Dataset  \"\"\"\n",
    "    train_dir = pathlib.Path(train_dir).with_suffix('') \n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=dataset_config.labels,\n",
    "        color_mode=dataset_config.color_mode,\n",
    "        image_size=dataset_config.image_size,\n",
    "        shuffle=dataset_config.shuffle,\n",
    "        validation_split=dataset_config.validation_split,\n",
    "        subset=subset,\n",
    "        interpolation=dataset_config.interpolation,\n",
    "        seed=dataset_config.seed,\n",
    "        batch_size=dataset_config.batch_size\n",
    "    )\n",
    "train_ds = build_dataset(train_dir=adapter.train_dir, subset=\"training\", dataset_config=config.dataset)\n",
    "val_ds = build_dataset(train_dir=adapter.train_dir, subset=\"validation\", dataset_config=config.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "train_ds = (train_ds\n",
    "            .cache()\n",
    "            .shuffle(buffer_size=len(train_ds))            \n",
    "            .map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Callbacks\n",
    "Four callbacks are defined for the training phase. \n",
    "1. Early Stop: To mitigate overfitting caused by excessive training sessions, we stop training once validation loss hasn't improved in a designated number of epochs. \n",
    "2. Learning Rate Callback: If validation loss hasn't improved in the designated number of epochs (3), the learning rate is reduced by a factor of 0.5.\n",
    "3. Model Checkpoint: A model checkpoint is taken when the validation accuracy has improved. The state of the model at the best validation accuracy score are restored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(config: Config) -> list:\n",
    "    \"\"\"Construct an early stop, learning rate, and model checkpoint callback. \"\"\"    \n",
    "    \n",
    "    lr_range_test_callback = LRRangeTestCallback(min_lr=config.learning_rate_schedule.min_lr,\n",
    "                                         max_lr=config.learning_rate_schedule.max_lr,\n",
    "                                         epochs=config.learning_rate_schedule.epochs)\n",
    "    \n",
    "    return [lr_range_test_callback]\n",
    "\n",
    "with strategy.scope():\n",
    "    callbacks = build_callbacks(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = TMNetFactory(config=config.network)\n",
    "repo = ExperimentRepo(mode=mode, project=config.project.name, adapter=adapter)\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "with strategy.scope():\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "The model is encapsulated in a Network object that also contains metadata, the base model, and the network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================================================ #\n",
      "                                           TMNet_DenseNet                                           \n",
      "# ------------------------------------------------------------------------------------------------ #\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TMNet_DenseNet_input_layer   [(None, 224, 224, 3)]    0         \n",
      " (InputLayer)                                                    \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)      0         \n",
      "                                                                 \n",
      " tf.math.truediv_1 (TFOpLamb  (None, 224, 224, 3)      0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " densenet201 (Functional)    (None, None, None, 1920)  18321984  \n",
      "                                                                 \n",
      " TMNet_DenseNet_global_avera  (None, 1920)             0         \n",
      " ge_pooling (GlobalAveragePo                                     \n",
      " oling2D)                                                        \n",
      "                                                                 \n",
      " TMNet_DenseNet_batch_normal  (None, 1920)             7680      \n",
      " ization (BatchNormalization                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " TMNet_DenseNet_dense_1 (Den  (None, 1048)             2013208   \n",
      " se)                                                             \n",
      "                                                                 \n",
      " TMNet_DenseNet_dense_2 (Den  (None, 1048)             1099352   \n",
      " se)                                                             \n",
      "                                                                 \n",
      " TMNet_DenseNet_output_layer  (None, 1)                1049      \n",
      "  (Dense)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,443,273\n",
      "Trainable params: 3,117,449\n",
      "Non-trainable params: 18,325,824\n",
      "_________________________________________________________________\n",
      "# ------------------------------------------------------------------------------------------------ #\n",
      "\n",
      "\n",
      "                                            TMNetConfig                                             \n",
      "                                        Activation | sigmoid\n",
      "                                      Output Shape | 1\n",
      "                                            Dense1 | 1048\n",
      "                                            Dense2 | 1024\n",
      "\n",
      "\n",
      "# ================================================================================================ #\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/john/projects/bcd/wandb/run-20240218_033805-h2futyyy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test/runs/h2futyyy' target=\"_blank\">TMNet_DenseNet</a></strong> to <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test' target=\"_blank\">https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test/runs/h2futyyy' target=\"_blank\">https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test/runs/h2futyyy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:LRRangeTestCallback:Setting learning rate to 1e-05.\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 60s 6s/step - loss: 0.6645 - accuracy: 0.5928 - auc: 0.5629 - precision: 0.4490 - recall: 0.2588 - val_loss: 0.6669 - val_accuracy: 0.6000 - val_auc: 0.6210 - val_precision: 0.5556 - val_recall: 0.4167\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.011120000000000001.\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 38s 6s/step - loss: 3.5258 - accuracy: 0.5928 - auc: 0.5519 - precision: 0.4561 - recall: 0.3059 - val_loss: 1.5571 - val_accuracy: 0.4182 - val_auc: 0.3911 - val_precision: 0.4259 - val_recall: 0.9583\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.022230000000000003.\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 37s 5s/step - loss: 0.8093 - accuracy: 0.6652 - auc: 0.6897 - precision: 0.5591 - recall: 0.6118 - val_loss: 0.8297 - val_accuracy: 0.6000 - val_auc: 0.4845 - val_precision: 0.5357 - val_recall: 0.6250\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.03334000000000001.\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 37s 6s/step - loss: 0.8790 - accuracy: 0.6018 - auc: 0.6230 - precision: 0.4805 - recall: 0.4353 - val_loss: 0.8467 - val_accuracy: 0.4182 - val_auc: 0.2419 - val_precision: 0.4259 - val_recall: 0.9583\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.04445000000000001.\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 37s 6s/step - loss: 0.8336 - accuracy: 0.6063 - auc: 0.5993 - precision: 0.4808 - recall: 0.2941 - val_loss: 0.6913 - val_accuracy: 0.5273 - val_auc: 0.5020 - val_precision: 0.4286 - val_recall: 0.2500\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.05556000000000001.\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 37s 5s/step - loss: 0.7350 - accuracy: 0.6561 - auc: 0.6737 - precision: 0.5714 - recall: 0.4235 - val_loss: 1.8574 - val_accuracy: 0.5636 - val_auc: 0.4980 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.06667000000000001.\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 38s 6s/step - loss: 0.8265 - accuracy: 0.5928 - auc: 0.5781 - precision: 0.4390 - recall: 0.2118 - val_loss: 0.6890 - val_accuracy: 0.5636 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.07778.\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 37s 5s/step - loss: 0.6986 - accuracy: 0.6018 - auc: 0.4455 - precision: 0.2000 - recall: 0.0118 - val_loss: 0.6851 - val_accuracy: 0.5636 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.08889000000000001.\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 37s 5s/step - loss: 0.6727 - accuracy: 0.6154 - auc: 0.4402 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6886 - val_accuracy: 0.5636 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "INFO:LRRangeTestCallback:Setting learning rate to 0.1.\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 37s 5s/step - loss: 0.6659 - accuracy: 0.6154 - auc: 0.5168 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6932 - val_accuracy: 0.5636 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb.plots.* functions are deprecated and will be removed in a future release. Please use wandb.plot.* instead.\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f26e631074f4b01b3d93c5277b77df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▁█▂▂▇▁▂▃▃</td></tr><tr><td>epoch/auc</td><td>▄▄█▆▅█▅▁▁▃</td></tr><tr><td>epoch/epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch/loss</td><td>▁█▁▂▁▁▁▁▁▁</td></tr><tr><td>epoch/precision</td><td>▆▇█▇▇█▆▃▁▁</td></tr><tr><td>epoch/recall</td><td>▄▅█▆▄▆▃▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>█▁█▁▅▇▇▇▇▇</td></tr><tr><td>epoch/val_auc</td><td>█▄▅▁▆▆▆▆▆▆</td></tr><tr><td>epoch/val_loss</td><td>▁▆▂▂▁█▁▁▁▁</td></tr><tr><td>epoch/val_precision</td><td>█▆█▆▆▁▁▁▁▁</td></tr><tr><td>epoch/val_recall</td><td>▄█▆█▃▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.61538</td></tr><tr><td>epoch/auc</td><td>0.51683</td></tr><tr><td>epoch/epoch</td><td>9</td></tr><tr><td>epoch/learning_rate</td><td>0.1</td></tr><tr><td>epoch/loss</td><td>0.66595</td></tr><tr><td>epoch/precision</td><td>0.0</td></tr><tr><td>epoch/recall</td><td>0.0</td></tr><tr><td>epoch/val_accuracy</td><td>0.56364</td></tr><tr><td>epoch/val_auc</td><td>0.5</td></tr><tr><td>epoch/val_loss</td><td>0.69316</td></tr><tr><td>epoch/val_precision</td><td>0.0</td></tr><tr><td>epoch/val_recall</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TMNet_DenseNet</strong> at: <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test/runs/h2futyyy' target=\"_blank\">https://wandb.ai/aistudio/Breast-Cancer-Detection-Development-Learning-Rate-Range-Test/runs/h2futyyy</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240218_033805-h2futyyy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with strategy.scope():  \n",
    "    network = factory.create(base_model=base_model)\n",
    "    # Tags allowing models and runs to be searched on Weights and Biases\n",
    "    tags = [\"CPU\", network.name, network.architecture, base_model.name, \"lr_range_test\"]        \n",
    "    experiment = FeatureExtractionExperiment(\n",
    "        network=network, \n",
    "        config=config, \n",
    "        optimizer=optimizer, \n",
    "        repo=repo, \n",
    "        metrics=metrics, \n",
    "        callbacks=callbacks, checkpoint=False, tags=tags, force=force)\n",
    "    experiment.run(train_ds=train_ds, val_ds=val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
