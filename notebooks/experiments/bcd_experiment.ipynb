{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from bcd.model.network.base import NetworkConfig\n",
    "from bcd.model.network.tmnet import TMNetConfig, TMNetFactory\n",
    "from bcd.model.network.aknet import AKNetConfig, AKNetFactory\n",
    "from bcd.model.network.shainnet import ShainNetConfig, ShainNetFactory\n",
    "from bcd.model.network.simplenet import SimpleNetConfig, SimpleNetFactory\n",
    "from bcd.model.network.simplenetv2 import SimpleNetV2Config, SimpleNetV2Factory\n",
    "from bcd.model.network.tmnetv2 import TMNetV2Config, TMNetV2Factory\n",
    "from bcd.model.store import ExperimentRepo\n",
    "from bcd.model.callback import TriangleLearningRateScheduleCallback\n",
    "from bcd.model.pretrained import DenseNet, EfficientNet, Inception, InceptionResNet, MobileNet, ResNet, Xception\n",
    "from bcd.model.experiment import FeatureExtractionExperiment\n",
    "from bcd.model.config import ProjectConfig, DatasetConfig, CheckPointConfig, TrainConfig, EarlyStopConfig, LearningRateScheduleConfig, Config, ExperimentConfig\n",
    "from bcd.model.adapter import LocalAdapter, Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"Development\"\n",
    "force = False\n",
    "base_models = [DenseNet(), EfficientNet(), Inception(), InceptionResNet(), MobileNet(), ResNet(), Xception()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform \n",
    "The platform object encapsulates variables that are platform-dependent, such as device type, distribute strategy, api keys, file paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maistudio\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/john/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = LocalAdapter(mode=mode)\n",
    "print(f\"Running on {adapter.device_type}\")\n",
    "\n",
    "# Obtain the TensorFlow state and compute distribution policy, i.e. strategy\n",
    "strategy  = adapter.get_strategy()\n",
    "\n",
    "# Weights and Biases login for model and metric tracking.\n",
    "wandb.login(key=adapter.wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything():\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------ #\n",
    "aknet = AKNetConfig(dense1=2048,\n",
    "                    dropout1=0.5,\n",
    "                    dense2=2048,\n",
    "                    dropout2=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "shainnet = ShainNetConfig(dense1=1024, \n",
    "                           dropout1=0.5,\n",
    "                           dense2=1024,\n",
    "                           dropout2=0.3,\n",
    "                           dense3=512,\n",
    "                           dense4=128)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "simplenetv2 = SimpleNetV2Config(dense1=1024, \n",
    "                                dropout1=0.5)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "tmnet = TMNetConfig(dense1=512,\n",
    "                     dense2=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(adapter: Adapter, mode: str, strategy: tf.distribute.Strategy) -> Config:\n",
    "    \"\"\"Constructs an experiment Config object \"\"\"\n",
    "    # Encapsulates the parameters that define the project in Weights & Biases\n",
    "    project_config = ProjectConfig(mode=mode)\n",
    "\n",
    "    # The TMNet architecture has two dense layers before a sigmoid activation. We'll set the number of nodes in the last two dense layers to 1024 and 512 respectively. \n",
    "    network_config = TMNetConfig(\n",
    "        activation=\"sigmoid\", \n",
    "        input_shape=(224,224,3), \n",
    "        output_shape=1, \n",
    "        dense1=1048, \n",
    "        dense2=1024)\n",
    "\n",
    "    # The default batch size is 64; however, if running on TPU, the rule of thumb is to optimally set the batch size to 128 * the number of TPU cores    \n",
    "    batch_size = 64 if not adapter.device_type == \"TPU\" else 16 * strategy.num_replicas_in_sync\n",
    "    dataset_config = DatasetConfig(\n",
    "        mode=mode,        \n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # If running on TPU, the learning rate is scaled by the number of cores corresponding to the batch size.\n",
    "    learning_rate = 1e-4 if adapter.device_type != \"TPU\" else 1e-4 *  strategy.num_replicas_in_sync\n",
    "    train_config = TrainConfig(\n",
    "        epochs=10, \n",
    "        learning_rate=learning_rate)    \n",
    "\n",
    "    # Checkpoints will be stored in the directory given by the adapter object. \n",
    "    checkpoint_config = CheckPointConfig(\n",
    "        directory=adapter.model_dir, \n",
    "        monitor=\"val_accuracy\", \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"auto\")\n",
    "\n",
    "    # We'll establish an early stop callback to mitigate overfitting caused by excessive training after validation loss hasn't improved.\n",
    "    early_stop_config = EarlyStopConfig(\n",
    "        min_delta=1e-4, \n",
    "        monitor=\"val_loss\", \n",
    "        patience=10, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1)\n",
    "\n",
    "    # Parameters for the learning rate policy\n",
    "    learning_rate_schedule_config = LearningRateScheduleConfig(\n",
    "        min_lr=1e-5, \n",
    "        max_lr=1e-1\n",
    "        step_size=4)\n",
    "\n",
    "    # The experiment configuration is encapsulated into a single object \n",
    "    return ExperimentConfig(project=project_config, \n",
    "                    dataset=dataset_config, \n",
    "                    train=train_config, \n",
    "                    network=network_config, \n",
    "                    checkpoint=checkpoint_config, \n",
    "                    early_stop=early_stop_config, \n",
    "                    learning_rate_schedule=learning_rate_schedule_config)\n",
    "\n",
    "# Construct the configuration object.\n",
    "config = build_config(adapter=adapter, mode=mode, strategy=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 files belonging to 2 classes.\n",
      "Using 221 files for training.\n",
      "Found 276 files belonging to 2 classes.\n",
      "Using 55 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_dataset(train_dir: str, subset: str, dataset_config: Config, tpu: bool = False) -> tf.data.Dataset:\n",
    "    \"\"\"Produces a TensorFlow training or validation  Dataset  \"\"\"\n",
    "    train_dir = pathlib.Path(train_dir).with_suffix('') \n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=dataset_config.labels,\n",
    "        color_mode=dataset_config.color_mode,\n",
    "        image_size=dataset_config.image_size,\n",
    "        shuffle=dataset_config.shuffle,\n",
    "        validation_split=dataset_config.validation_split,\n",
    "        subset=subset,\n",
    "        interpolation=dataset_config.interpolation,\n",
    "        seed=dataset_config.seed,\n",
    "        batch_size=dataset_config.batch_size\n",
    "    )\n",
    "train_ds = build_dataset(train_dir=adapter.train_dir, subset=\"training\", dataset_config=config.dataset)\n",
    "val_ds = build_dataset(train_dir=adapter.train_dir, subset=\"validation\", dataset_config=config.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "train_ds = (train_ds\n",
    "            .cache()\n",
    "            .shuffle(buffer_size=len(train_ds))            \n",
    "            .map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Callbacks\n",
    "Four callbacks are defined for the training phase. \n",
    "1. Early Stop: To mitigate overfitting caused by excessive training sessions, we stop training once validation loss hasn't improved in a designated number of epochs. \n",
    "2. Learning Rate Callback: If validation loss hasn't improved in the designated number of epochs (3), the learning rate is reduced by a factor of 0.5.\n",
    "3. Model Checkpoint: A model checkpoint is taken when the validation accuracy has improved. The state of the model at the best validation accuracy score are restored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(config: Config) -> list:\n",
    "    \"\"\"Construct an early stop and learning rate callback. \"\"\"    \n",
    "    \n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=config.early_stop.monitor, \n",
    "                                                        min_delta=config.early_stop.min_delta,\n",
    "                                                        patience=config.early_stop.patience, \n",
    "                                                        restore_best_weights=config.early_stop.restore_best_weights,\n",
    "                                                        verbose=config.early_stop.verbose)\n",
    "    \n",
    "    learning_rate_schedule_callback = TriangleLearningRateScheduleCallback(\n",
    "        min_lr=config.learning_rate_schedule.min_lr,\n",
    "        max_lr=config.learning_rate_schedule.max_lr,\n",
    "        step_size=config.learning_rate_schedule.step_size\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return [early_stop_callback, learning_rate_schedule_callback]\n",
    "\n",
    "with strategy.scope():\n",
    "    callbacks = build_callbacks(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Several dependencies are instantiated / declared:\n",
    "\n",
    "1. The model factory exposes a create method that constructs a model for the factory's architecture and a pretrained base model. \n",
    "2. The repository controls the ways in which models are persisted on Kaggle and Weights & Biases.\n",
    "3. The optimizer is the algorithm that tunes the weights of the model to minimize the loss function.\n",
    "4. Designate the metrics to compute during training with in the TensorFlow strategy context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = TMNetFactory(config=config.network)\n",
    "repo = ExperimentRepo(mode=mode, project=config.project.name, adapter=adapter)\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "with strategy.scope():\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================================================ #\n",
      "                                           TMNet_DenseNet                                           \n",
      "# ------------------------------------------------------------------------------------------------ #\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TMNet_DenseNet_input_layer   [(None, 224, 224, 3)]    0         \n",
      " (InputLayer)                                                    \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)      0         \n",
      "                                                                 \n",
      " tf.math.truediv_1 (TFOpLamb  (None, 224, 224, 3)      0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " densenet201 (Functional)    (None, None, None, 1920)  18321984  \n",
      "                                                                 \n",
      " TMNet_DenseNet_global_avera  (None, 1920)             0         \n",
      " ge_pooling (GlobalAveragePo                                     \n",
      " oling2D)                                                        \n",
      "                                                                 \n",
      " TMNet_DenseNet_batch_normal  (None, 1920)             7680      \n",
      " ization (BatchNormalization                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " TMNet_DenseNet_dense_1 (Den  (None, 1048)             2013208   \n",
      " se)                                                             \n",
      "                                                                 \n",
      " TMNet_DenseNet_dense_2 (Den  (None, 1048)             1099352   \n",
      " se)                                                             \n",
      "                                                                 \n",
      " TMNet_DenseNet_output_layer  (None, 1)                1049      \n",
      "  (Dense)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,443,273\n",
      "Trainable params: 3,117,449\n",
      "Non-trainable params: 18,325,824\n",
      "_________________________________________________________________\n",
      "# ------------------------------------------------------------------------------------------------ #\n",
      "\n",
      "\n",
      "                                            TMNetConfig                                             \n",
      "                                        Activation | sigmoid\n",
      "                                      Output Shape | 1\n",
      "                                            Dense1 | 1048\n",
      "                                            Dense2 | 1024\n",
      "\n",
      "\n",
      "# ================================================================================================ #\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/john/projects/bcd/wandb/run-20240217_232050-xx7ulne3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development/runs/xx7ulne3' target=\"_blank\">TMNet_DenseNet</a></strong> to <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development' target=\"_blank\">https://wandb.ai/aistudio/Breast-Cancer-Detection-Development</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development/runs/xx7ulne3' target=\"_blank\">https://wandb.ai/aistudio/Breast-Cancer-Detection-Development/runs/xx7ulne3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 62s 12s/step - loss: 0.7024 - val_loss: 0.6930 - lr: 1.0000e-04\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 39s 10s/step - loss: 0.6398 - val_loss: 0.7343 - lr: 1.0000e-04\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 40s 10s/step - loss: 0.5966 - val_loss: 0.7684 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb.plots.* functions are deprecated and will be removed in a future release. Please use wandb.plot.* instead.\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0430246fa53b4e37839c9bc8373b61fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.010 MB of 0.010 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/epoch</td><td>▁▅█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▁</td></tr><tr><td>epoch/lr</td><td>▁▁▁</td></tr><tr><td>epoch/val_loss</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/epoch</td><td>2</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.59663</td></tr><tr><td>epoch/lr</td><td>0.0001</td></tr><tr><td>epoch/val_loss</td><td>0.76841</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TMNet_DenseNet</strong> at: <a href='https://wandb.ai/aistudio/Breast-Cancer-Detection-Development/runs/xx7ulne3' target=\"_blank\">https://wandb.ai/aistudio/Breast-Cancer-Detection-Development/runs/xx7ulne3</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240217_232050-xx7ulne3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for base_model in base_models:\n",
    "    with strategy.scope():  \n",
    "        network = factory.create(base_model=base_model)\n",
    "        # Tags allowing models and runs to be searched on Weights and Biases\n",
    "        tags = [\"CPU\", network.name, network.architecture, base_model.name]        \n",
    "        experiment = FeatureExtractionExperiment(\n",
    "            network=network, \n",
    "            config=config, \n",
    "            optimizer=optimizer, \n",
    "            repo=repo, \n",
    "            metrics=metrics, \n",
    "            callbacks=callbacks, checkpoint=False, tags=tags, force=force)\n",
    "        experiment.run(train_ds=train_ds, val_ds=val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
