{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from bcd.model.network.base import NetworkConfig\n",
    "from bcd.model.network.tmnet import TMNetConfig, TMNetFactory\n",
    "from bcd.model.store import ExperimentRepo\n",
    "from bcd.model.pretrained import DenseNet, EfficientNet, Inception, InceptionResNet, MobileNet, ResNet, Xception\n",
    "from bcd.model.experiment import FeatureExtractionExperiment\n",
    "from bcd.model.config import ProjectConfig, DatasetConfig, CheckPointConfig, TrainConfig, EarlyStopConfig, LearningRateScheduleConfig, Config, ExperimentConfig\n",
    "from bcd.model.adapter import LocalAdapter, Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"Development\"\n",
    "force = True\n",
    "base_model = DenseNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform \n",
    "The platform object encapsulates variables that are platform-dependent, such as device type, distribute strategy, api keys, file paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maistudio\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/john/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = LocalAdapter(mode=mode)\n",
    "print(f\"Running on {adapter.device_type}\")\n",
    "\n",
    "# Obtain the TensorFlow state and compute distribution policy, i.e. strategy\n",
    "strategy  = adapter.get_strategy()\n",
    "\n",
    "# Weights and Biases login for model and metric tracking.\n",
    "wandb.login(key=adapter.wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything():\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(adapter: Adapter, mode: str, strategy: tf.distribute.Strategy) -> Config:\n",
    "    \"\"\"Constructs an experiment Config object \"\"\"\n",
    "    # Encapsulates the parameters that define the project in Weights & Biases\n",
    "    project_config = ProjectConfig(mode=mode)\n",
    "\n",
    "    # The TMNet architecture has two dense layers before a sigmoid activation. We'll set the number of nodes in the last two dense layers to 1024 and 512 respectively. \n",
    "    network_config = TMNetConfig(\n",
    "        activation=\"sigmoid\", \n",
    "        input_shape=(224,224,3), \n",
    "        output_shape=1, \n",
    "        dense1=1048, \n",
    "        dense2=1024)\n",
    "\n",
    "    # The default batch size is 64; however, if running on TPU, the rule of thumb is to optimally set the batch size to 128 * the number of TPU cores    \n",
    "    batch_size = 64 if not adapter.device_type == \"TPU\" else 16 * strategy.num_replicas_in_sync\n",
    "    dataset_config = DatasetConfig(\n",
    "        mode=mode,        \n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # If running on TPU, the learning rate is scaled by the number of cores corresponding to the batch size.\n",
    "    learning_rate = 1e-4 if adapter.device_type != \"TPU\" else 1e-4 *  strategy.num_replicas_in_sync\n",
    "    train_config = TrainConfig(\n",
    "        epochs=3, \n",
    "        learning_rate=learning_rate)    \n",
    "\n",
    "    # Checkpoints will be stored in the directory given by the adapter object. \n",
    "    checkpoint_config = CheckPointConfig(\n",
    "        directory=adapter.model_dir, \n",
    "        monitor=\"val_accuracy\", \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"auto\")\n",
    "\n",
    "    # We'll establish an early stop callback to mitigate overfitting caused by excessive training after validation loss hasn't improved.\n",
    "    early_stop_config = EarlyStopConfig(\n",
    "        min_delta=1e-4, \n",
    "        monitor=\"val_loss\", \n",
    "        patience=10, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1)\n",
    "\n",
    "    # Learning rate will be annealed by a factor of 0.5 if validation loss hasn't improved in 3 epochs.\n",
    "    learning_rate_schedule_config = LearningRateScheduleConfig(\n",
    "        min_delta=1e-4, \n",
    "        monitor=\"val_loss\", \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode=\"auto\")\n",
    "\n",
    "    # The experiment configuration is encapsulated into a single object \n",
    "    return ExperimentConfig(project=project_config, \n",
    "                    dataset=dataset_config, \n",
    "                    train=train_config, \n",
    "                    network=network_config, \n",
    "                    checkpoint=checkpoint_config, \n",
    "                    early_stop=early_stop_config, \n",
    "                    learning_rate_schedule=learning_rate_schedule_config)\n",
    "\n",
    "# Construct the configuration object.\n",
    "config = build_config(adapter=adapter, mode=mode, strategy=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 files belonging to 2 classes.\n",
      "Using 221 files for training.\n",
      "Found 276 files belonging to 2 classes.\n",
      "Using 55 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_dataset(train_dir: str, subset: str, dataset_config: Config, tpu: bool = False) -> tf.data.Dataset:\n",
    "    \"\"\"Produces a TensorFlow training or validation  Dataset  \"\"\"\n",
    "    train_dir = pathlib.Path(train_dir).with_suffix('') \n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=dataset_config.labels,\n",
    "        color_mode=dataset_config.color_mode,\n",
    "        image_size=dataset_config.image_size,\n",
    "        shuffle=dataset_config.shuffle,\n",
    "        validation_split=dataset_config.validation_split,\n",
    "        subset=subset,\n",
    "        interpolation=dataset_config.interpolation,\n",
    "        seed=dataset_config.seed,\n",
    "        batch_size=dataset_config.batch_size\n",
    "    )\n",
    "train_ds = build_dataset(train_dir=adapter.train_dir, subset=\"training\", dataset_config=config.dataset)\n",
    "val_ds = build_dataset(train_dir=adapter.train_dir, subset=\"validation\", dataset_config=config.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "train_ds = (train_ds\n",
    "            .cache()\n",
    "            .shuffle(buffer_size=len(train_ds))            \n",
    "            .map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Callbacks\n",
    "Four callbacks are defined for the training phase. \n",
    "1. Early Stop: To mitigate overfitting caused by excessive training sessions, we stop training once validation loss hasn't improved in a designated number of epochs. \n",
    "2. Learning Rate Callback: If validation loss hasn't improved in the designated number of epochs (3), the learning rate is reduced by a factor of 0.5.\n",
    "3. Model Checkpoint: A model checkpoint is taken when the validation accuracy has improved. The state of the model at the best validation accuracy score are restored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(config: Config) -> list:\n",
    "    \"\"\"Construct an early stop, learning rate, and model checkpoint callback. \"\"\"    \n",
    "    \n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=config.early_stop.monitor, \n",
    "                                                        min_delta=config.early_stop.min_delta,\n",
    "                                                        patience=config.early_stop.patience, \n",
    "                                                        restore_best_weights=config.early_stop.restore_best_weights,\n",
    "                                                        verbose=config.early_stop.verbose)\n",
    "\n",
    "    reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=config.learning_rate_schedule.monitor,\n",
    "                                                            factor=config.learning_rate_schedule.factor,\n",
    "                                                            patience=config.learning_rate_schedule.patience,\n",
    "                                                            verbose=config.learning_rate_schedule.verbose,\n",
    "                                                            mode=config.learning_rate_schedule.mode,\n",
    "                                                            min_delta=config.learning_rate_schedule.min_delta,\n",
    "                                                            min_lr=config.learning_rate_schedule.min_lr)\n",
    "\n",
    "    return [early_stop_callback, reduce_lr_callback]\n",
    "\n",
    "with strategy.scope():\n",
    "    callbacks = build_callbacks(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "The model is encapsulated in a Network object that also contains metadata, the base model, and the network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    factory = TMNetFactory(config=config.network)\n",
    "    network = factory.create(base_model=base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FeatureExtractionExperiment.run() got an unexpected keyword argument 'checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m repo \u001b[38;5;241m=\u001b[39m ExperimentRepo(mode\u001b[38;5;241m=\u001b[39mmode, project\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproject\u001b[38;5;241m.\u001b[39mname, adapter\u001b[38;5;241m=\u001b[39madapter)    \n\u001b[1;32m     10\u001b[0m experiment \u001b[38;5;241m=\u001b[39m FeatureExtractionExperiment(network\u001b[38;5;241m=\u001b[39mnetwork, config\u001b[38;5;241m=\u001b[39mconfig, optimizer\u001b[38;5;241m=\u001b[39moptimizer, repo\u001b[38;5;241m=\u001b[39mrepo, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, notes\u001b[38;5;241m=\u001b[39mnotes, tags\u001b[38;5;241m=\u001b[39mtags, force\u001b[38;5;241m=\u001b[39mforce)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: FeatureExtractionExperiment.run() got an unexpected keyword argument 'checkpoint'"
     ]
    }
   ],
   "source": [
    "# Any notes to associate with this experiment are included below\n",
    "notes = \"\"  \n",
    "# Tags allowing models and runs to be searched on Weights and Biases\n",
    "tags = [\"TPU\", network.name, network.architecture, base_model.name]\n",
    "# The Adam optimizer will be used for this experiment\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "# The experiment repository manages the way models are persisted locally and remotely on Weights and Biases servers.\n",
    "with strategy.scope():\n",
    "    repo = ExperimentRepo(mode=mode, project=config.project.name, adapter=adapter)    \n",
    "    experiment = FeatureExtractionExperiment(network=network, config=config, optimizer=optimizer, repo=repo, callbacks=callbacks, notes=notes, tags=tags, checkpoint=False, force=force)\n",
    "    experiment.run(train_ds=train_ds, val_ds=val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
