{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from bcd.model.network.base import NetworkConfig\n",
    "from bcd.model.network.tmnet import TMNetConfig, TMNetFactory\n",
    "from bcd.model.network.aknet import AKNetConfig, AKNetFactory\n",
    "from bcd.model.network.shainnet import ShainNetConfig, ShainNetFactory\n",
    "from bcd.model.network.simplenet import SimpleNetConfig, SimpleNetFactory\n",
    "from bcd.model.network.simplenetv2 import SimpleNetV2Config, SimpleNetV2Factory\n",
    "from bcd.model.network.tmnetv2 import TMNetV2Config, TMNetV2Factory\n",
    "from bcd.model.store import ExperimentRepo\n",
    "from bcd.model.callback import TriangleLearningRateScheduleCallback\n",
    "from bcd.model.pretrained import DenseNet, EfficientNet, Inception, InceptionResNet, MobileNet, ResNet, Xception\n",
    "from bcd.model.experiment import FeatureExtractionExperiment\n",
    "from bcd.model.config import ProjectConfig, DatasetConfig, CheckPointConfig, TrainConfig, EarlyStopConfig, LearningRateScheduleConfig, Config, ExperimentConfig\n",
    "from bcd.model.adapter import LocalAdapter, Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"Production\"\n",
    "force = False\n",
    "base_models = [DenseNet(), EfficientNet(), Inception(), InceptionResNet(), MobileNet(), ResNet(), Xception()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform \n",
    "The platform object encapsulates variables that are platform-dependent, such as device type, distribute strategy, api keys, file paths, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/john/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "Using CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = LocalAdapter(mode=mode)\n",
    "print(f\"Running on {adapter.device_type}\")\n",
    "\n",
    "# Obtain the TensorFlow state and compute distribution policy, i.e. strategy\n",
    "strategy  = adapter.get_strategy()\n",
    "\n",
    "# Weights and Biases login for model and metric tracking.\n",
    "wandb.login(key=adapter.wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything():\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------ #\n",
    "aknet = AKNetConfig(dense1=2048,\n",
    "                    dropout1=0.5,\n",
    "                    dense2=2048,\n",
    "                    dropout2=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "shainnet = ShainNetConfig(dense1=1024, \n",
    "                           dropout1=0.5,\n",
    "                           dense2=1024,\n",
    "                           dropout2=0.3,\n",
    "                           dense3=512,\n",
    "                           dense4=128)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "simplenetv2 = SimpleNetV2Config(dense1=1024, \n",
    "                                dropout1=0.5)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "tmnet = TMNetConfig(dense1=512,\n",
    "                     dense2=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(adapter: Adapter, mode: str, strategy: tf.distribute.Strategy) -> Config:\n",
    "    \"\"\"Constructs an experiment Config object \"\"\"\n",
    "    # Encapsulates the parameters that define the project in Weights & Biases\n",
    "    project_config = ProjectConfig(mode=mode)\n",
    "\n",
    "    # The TMNet architecture has two dense layers before a sigmoid activation. We'll set the number of nodes in the last two dense layers to 1024 and 512 respectively. \n",
    "    network_config = TMNetConfig(\n",
    "        activation=\"sigmoid\", \n",
    "        input_shape=(224,224,3), \n",
    "        output_shape=1, \n",
    "        dense1=1048, \n",
    "        dense2=1024)\n",
    "\n",
    "    # The default batch size is 64; however, if running on TPU, the rule of thumb is to optimally set the batch size to 128 * the number of TPU cores    \n",
    "    batch_size = 64 if not adapter.device_type == \"TPU\" else 16 * strategy.num_replicas_in_sync\n",
    "    dataset_config = DatasetConfig(\n",
    "        mode=mode,        \n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # If running on TPU, the learning rate is scaled by the number of cores corresponding to the batch size.\n",
    "    learning_rate = 1e-4 if adapter.device_type != \"TPU\" else 1e-4 *  strategy.num_replicas_in_sync\n",
    "    train_config = TrainConfig(\n",
    "        epochs=10, \n",
    "        learning_rate=learning_rate,\n",
    "        early_stop=True,\n",
    "        learning_rate_schedule=True,\n",
    "        augmentation=False,\n",
    "        checkpoint=True,\n",
    "    )        \n",
    "\n",
    "    # Checkpoints will be stored in the directory given by the adapter object. \n",
    "    checkpoint_config = CheckPointConfig(\n",
    "        directory=adapter.model_dir, \n",
    "        monitor=\"val_accuracy\", \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"auto\")\n",
    "\n",
    "    # We'll establish an early stop callback to mitigate overfitting caused by excessive training after validation loss hasn't improved.\n",
    "    early_stop_config = EarlyStopConfig(\n",
    "        min_delta=1e-4, \n",
    "        monitor=\"val_loss\", \n",
    "        patience=10, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1)\n",
    "\n",
    "    # Parameters for the learning rate policy\n",
    "    learning_rate_schedule_config = LearningRateScheduleConfig(\n",
    "        min_lr=1e-5, \n",
    "        max_lr=1e-1)\n",
    "\n",
    "    # The experiment configuration is encapsulated into a single object \n",
    "    return ExperimentConfig(project=project_config, \n",
    "                    dataset=dataset_config, \n",
    "                    train=train_config, \n",
    "                    network=network_config, \n",
    "                    checkpoint=checkpoint_config, \n",
    "                    early_stop=early_stop_config, \n",
    "                    learning_rate_schedule=learning_rate_schedule_config)\n",
    "\n",
    "# Construct the configuration object.\n",
    "config = build_config(adapter=adapter, mode=mode, strategy=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2471 files belonging to 2 classes.\n",
      "Using 1977 files for training.\n",
      "Found 2471 files belonging to 2 classes.\n",
      "Using 494 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def build_dataset(train_dir: str, subset: str, dataset_config: Config, tpu: bool = False) -> tf.data.Dataset:\n",
    "    \"\"\"Produces a TensorFlow training or validation  Dataset  \"\"\"\n",
    "    train_dir = pathlib.Path(train_dir).with_suffix('') \n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels=dataset_config.labels,\n",
    "        color_mode=dataset_config.color_mode,\n",
    "        image_size=dataset_config.image_size,\n",
    "        shuffle=dataset_config.shuffle,\n",
    "        validation_split=dataset_config.validation_split,\n",
    "        subset=subset,\n",
    "        interpolation=dataset_config.interpolation,\n",
    "        seed=dataset_config.seed,\n",
    "        batch_size=dataset_config.batch_size\n",
    "    )\n",
    "train_ds = build_dataset(train_dir=adapter.train_dir, subset=\"training\", dataset_config=config.dataset)\n",
    "val_ds = build_dataset(train_dir=adapter.train_dir, subset=\"validation\", dataset_config=config.dataset)\n",
    "y = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "y.sum() \n",
    "len(y)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-16 15:48:14.410300: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 334 of 512\n",
      "2024-03-16 15:48:19.744977: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1977"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "y.sum() \n",
    "len(y)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "train_ds = (train_ds\n",
    "            .cache()\n",
    "            .shuffle(buffer_size=len(train_ds))            \n",
    "            .map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Callbacks\n",
    "Four callbacks are defined for the training phase. \n",
    "1. Early Stop: To mitigate overfitting caused by excessive training sessions, we stop training once validation loss hasn't improved in a designated number of epochs. \n",
    "2. Learning Rate Callback: If validation loss hasn't improved in the designated number of epochs (3), the learning rate is reduced by a factor of 0.5.\n",
    "3. Model Checkpoint: A model checkpoint is taken when the validation accuracy has improved. The state of the model at the best validation accuracy score are restored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callbacks(config: Config) -> list:\n",
    "    \"\"\"Construct an early stop and learning rate callback. \"\"\"    \n",
    "    \n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor=config.early_stop.monitor, \n",
    "                                                        min_delta=config.early_stop.min_delta,\n",
    "                                                        patience=config.early_stop.patience, \n",
    "                                                        restore_best_weights=config.early_stop.restore_best_weights,\n",
    "                                                        verbose=config.early_stop.verbose)\n",
    "    \n",
    "    learning_rate_schedule_callback = TriangleLearningRateScheduleCallback(\n",
    "        min_lr=config.learning_rate_schedule.min_lr,\n",
    "        max_lr=config.learning_rate_schedule.max_lr,\n",
    "        step_size=config.learning_rate_schedule.step_size\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return [early_stop_callback, learning_rate_schedule_callback]\n",
    "\n",
    "with strategy.scope():\n",
    "    callbacks = build_callbacks(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Several dependencies are instantiated / declared:\n",
    "\n",
    "1. The model factory exposes a create method that constructs a model for the factory's architecture and a pretrained base model. \n",
    "2. The repository controls the ways in which models are persisted on Kaggle and Weights & Biases.\n",
    "3. The optimizer is the algorithm that tunes the weights of the model to minimize the loss function.\n",
    "4. Designate the metrics to compute during training with in the TensorFlow strategy context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = TMNetFactory(config=config.network)\n",
    "repo = ExperimentRepo(mode=mode, project=config.project.name, adapter=adapter)\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "with strategy.scope():\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run Experiment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for base_model in base_models:\n",
    "    with strategy.scope():  \n",
    "        network = factory.create(base_model=base_model)\n",
    "        # Tags allowing models and runs to be searched on Weights and Biases\n",
    "        tags = [\"CPU\", network.name, network.architecture, base_model.name]        \n",
    "        experiment = FeatureExtractionExperiment(\n",
    "            network=network, \n",
    "            config=config, \n",
    "            optimizer=optimizer, \n",
    "            repo=repo, \n",
    "            metrics=metrics, \n",
    "            callbacks=callbacks, tags=tags, force=force)\n",
    "        experiment.run(train_ds=train_ds, val_ds=val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
