{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d805b2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Precise and accurate diagnosis of breast cancer rests upon the discriminatory power of mathematical models designed to detect and classify structural abnormalities in breast tissue from biomedical imaging. Advances in artificial intelligence and computer vision, fueled by an explosion in AI task-specific computational power, have given rise to dense image recognition models capable of distinguishing increasingly complex patterns and structures in biomedical images. Still, the diagnostic performance and clinical applicability of such models rests upon the availability of large datasets containing high-quality, high-resolution images that are clear, sharp, and free of noise and artifacts.\n",
    "\n",
    "Exploratory analysis of the CBIS-DDSM mammograph illuminated several issues that compromise the discriminatory power of image detection and recognition models.\n",
    "\n",
    "- Various artifacts (large texts and annotations) are present within the mammography that resemble the pixel intensities of the regions of interest (ROIs), which can interfere with the ROI extraction process and/or lead to false diagnosis.\n",
    "- Noise of various types in the images is an obstacle to effective feature extraction, image detection, recognition, and classification.\n",
    "- Poor brightness and contrast levels in some mammograms may increase the influence of noise, and/or conceal important and subtle features.\n",
    "- Malignant tumors are characterized by irregular shapes and ambiguous or blurred edges that complicate the ROI segmentation task.\n",
    "- Dense breast tissue with pixel intensities similar to that of cancerous tissue, may conceal subtle structures of diagnostic importance.\n",
    "- Deep learning models for computer vision require large datasets. The CBIS-DDSM has just over 3500 full mammogram images, a relatively small dataset for model training.\n",
    "\n",
    "Addressing these challenges is fundamentally important to model detection, recognition, and classification performance.\n",
    "\n",
    "## Image Preprocessing Approach\n",
    "\n",
    "In this regard, a five-stage image preprocessing approach ({numref}`image_prep`) has been devised to reduce noise in the images, eliminate artifacts, and produce a collection of images for maximally effective computer vision model training and classification.\n",
    "\n",
    "```{figure} ../../figures/ImagePrep.png\n",
    "---\n",
    "name: image_prep\n",
    "---\n",
    "Image Preprocessing Approach\n",
    "\n",
    "```\n",
    "\n",
    "We begin with an evaluation of various denoising methods commonly applied to mammography. Once a denoising method and its (hyper) parameters are selected, we move to the artifact removal stage. Image binarizing and thresholding methods are evaluated, then morphological transformations are applied to the binarized images to remove artifacts. Next, the pectoral muscle is removed using various techniques such as Canny Edge Detection, Hough Lines Transformation, and Largest Contour Detection algorithms. To make malignant lesions more conspicuous during model training, we enhance image brightness and contrast with Gamma Correction and Contrast Limited Adaptive Histogram Equalization (CLAHE). Additive White Gaussian Noise (AWGN) is also added to improve the generalized performance of the neural network and mitigate model overfitting. Finally, we extract the ROIs using automated pixel intensity thresholding to create a binary mask which is applied to the enhanced images.\n",
    "\n",
    "## Image Quality Assessment (IQA)\n",
    "\n",
    "Image Quality Assessment (IQA) is the process of evaluating the extent to which an image preserves the information and appearance of the original image.  During the preprocessing stage, we will be evaluating various image enhancement methods in terms of the quality of the images they render. In this regard, our IQA will be a combination of subjective and objective assessment methods.\n",
    "\n",
    "### Subjective Image Quality Assessment\n",
    "\n",
    "Subjective methods are based on human subjects’ opinions of image quality {cite}`linLargeScaleCrowdsourcedSubjective2022`. Though time-consuming, expensive, and difficult to scale in real-time,\n",
    "subjective methods are considered the most reliable methods for assessing image quality because they rely on the opinions of human subjects, who represent the ultimate users of the digital media application {cite}`afnanSubjectiveAssessmentObjective2023`.\n",
    "\n",
    "Subjective assessments will be performed on a double stimulus comparison scale (DSCS), in which a random selection of test images is compared to its associated ground truth images. In addition,\n",
    "images from the various image enhancement methods are visually evaluated and scored in terms of relative image quality.\n",
    "\n",
    "### Objective Image Quality Assessment\n",
    "\n",
    "Objective methods are based on image quality metrics (IQMs) that are designed to estimate the quality of the image automatically, in qualitative terms as observed by human subjects {cite}`afnanSubjectiveAssessmentObjective2023`. Image processing algorithms will be ranked by calculating these metrics to select the algorithm that produces the highest-quality images.\n",
    "\n",
    "For this effort, three IQMs will be used to assess image quality: mean squared error (MSE), peak signal-to-noise ratio (PSNR), and the structural similarity index measure (SSIM).\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "MSE, the most common performance criterion for measuring image quality, characterizes the squared error among pixels contained in two images. Mathematically, MSE is defined as follows:\n",
    "\n",
    "```{math}\n",
    ":label: mse\n",
    "\\text{MSE} = \\frac{1}{mn}\\displaystyle\\sum_{i=0}^{m-1}\\displaystyle\\sum_{j=0}^{n-1}(G(i,j)-P(i,j))^2\n",
    "```\n",
    "\n",
    "where, **G** is the ground truth image, and **P** is the processed image. The pixels of **G** and **P** are denoted by **m** and **n**. Lastly, the rows and columns of pixels **m**, and **n**, are denoted by **i**, and **j**.\n",
    "\n",
    "The objective here is to find the *minimum mean-squared error estimator* (MMSE).\n",
    "\n",
    "Though the use of MSE is widespread, it does have a shortcoming. The main problem with mean-squared error is that the measurement depends upon the image intensity scaling. An MSE of 100 for an 8-bit image with pixel values in the range 0-255 is objectionable, but an MSE of 100 for a 16-bit image with pixel values in [0,65536] would be barely noticeable.\n",
    "\n",
    "Peak Signal-to-Noise Ratio (PSNR) avoids this problem by scaling the MSE according to the image range.\n",
    "\n",
    "#### Peak Signal-to-Noise Ratio (PSNR)\n",
    "\n",
    "PSNR expresses the ratio between a signal’s maximum possible value (power) and the power of the corrupting noise that affects the quality of its representation.  The higher the PSNR, the smaller the error, and the better the quality of the image vis-a-vis the original.\n",
    "\n",
    "Mathematically, PSNR is calculated with equation {eq}`psnr`.\n",
    "\n",
    "```{math}\n",
    ":label: psnr\n",
    "\\text{PSNR} = 20\\text{log}_{10}\\Bigg(\\frac{(MAX)}{\\sqrt{MSE}}\\Bigg)\n",
    "```\n",
    "\n",
    "where MAX is the maximum pixel value contained in the image and PSNR is measured in decibels (dB). An acceptable PSNR for an 8-bit image would be in the range of 30 to 50 dB {cite}`beeravoluPreprocessingBreastCancer2021`.\n",
    "\n",
    "#### Structural Similarity Index Measure (SSIM)\n",
    "\n",
    "The Structural Similarity Index Measure (SSIM) is a method for measuring the structural similarity between two images. Since its introduction in 2004 {cite}`bakurovStructuralSimilarityIndex2022`, the SSIM has become one of the most popular full-reference image quality assessment (FR-IQA) measures (over 47,960 citations), owing its success to its mathematical simplicity, low computational complexity, and implicit incorporation of the Human Visual System’s (HVS) characteristics. The incorporation of these characteristics has resulted in a metric with better correlation with subjective evaluation provided by human observers. Consequently, SSIM has been used as a proxy evaluation for human assessment in a range of image processing and computer vision applications.\n",
    "\n",
    "SSIM separately measures local brightness (a.k.a. luminance), contrast, and structure of both images and then aggregates all local assessments to obtain the overall measure {cite}`bakurovStructuralSimilarityIndex2022`.\n",
    "\n",
    "Unlike the MSE-based measures that compare images on a pixel-by-pixel basis, the SSIM operates on patches obtained from a sliding window.  This technique better models the HVS because our eyes can discern differences in local information in a specific area of two images.\n",
    "\n",
    "Formally, the SSIM compares a reference image and a potentially corrupt image based on three independent components: luminance, contrast, and structure.\n",
    "\n",
    "##### SSIM Luminance Component\n",
    "\n",
    "Each image’s patch average $\\mu$ represents the luminance information. Hence, the comparison is given by:\n",
    "\n",
    "```{math}\n",
    ":label: luminance\n",
    "l(x,y) = \\frac{2\\mu_x\\mu_y+C_1}{\\mu_x^2 + \\mu_y^2 + C_1},\n",
    "```\n",
    "\n",
    "where $C_1$ is a small quantity introduced for numerical stability. (Stand by, we’ll define that quantity at the end.)\n",
    "\n",
    "##### SSIM Contrast Component\n",
    "\n",
    "Contrast is defined in terms of the standard deviation, and the comparison is given by:\n",
    "\n",
    "```{math}\n",
    ":label: contrast\n",
    "c(x,y) = \\frac{2\\sigma_x\\sigma_y+C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2},\n",
    "```\n",
    "\n",
    "##### SSIM Structure Component\n",
    "\n",
    "The structure element is represented by the inner product of the mean and standard deviation of each image. The comparison is given by:\n",
    "\n",
    "```{math}\n",
    ":label: structure\n",
    "s(x,y) = \\frac{\\sigma_{xy}+C_3}{\\sigma_x \\sigma_y + C_3}\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "```{math}\n",
    ":label: structure_var\n",
    "\\sigma_{xy} = \\frac{1}{N-1}\\displaystyle\\sum_{i=1}^N (x_i-\\mu_x)(y_i-\\mu_y).\n",
    "```\n",
    "\n",
    "Finally, the three components are combined into a unique expression that is weighted with exponents $\\alpha$, $\\beta$, and $\\gamma$:\n",
    "\n",
    "```{math}\n",
    ":label: ssim\n",
    "SSIM(x,y) = [l(x,y)]^\\alpha \\cdot [c(x,y)]^\\beta \\cdot [s(x,y)]^\\gamma\n",
    "```\n",
    "\n",
    "The expressions refer to constants $C_1$, $C_2$, and $C_3$ that are introduced for numerical stability. These three quantities are functions of the dynamic range of the pixel values L (L=255 for 8-bit gray-scale images) and two scalar constants $K_1 \\ll 1$ and $K_2 \\ll 1$. Traditionally, $K_1$  and $K_2$ are equal to 0.01, and 0.03, respectively. $C_1=(K_1,L)^2, C_2=(K_2,L)^2,  C_3=\\frac{C_2}{2}$. In the original paper {cite}`wangImageQualityAssessment2004`, $\\alpha= \\beta = \\gamma = 1$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.1"
   }
  },
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}