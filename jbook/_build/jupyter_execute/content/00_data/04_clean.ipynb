{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349e0bfe",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The prior data quality analysis revealed several data anomalies requiring attention. \n",
    "\n",
    "1. Cases with invalid values for subtlety and breast density.\n",
    "2. Missing calcification type, calcification distribution, mass shape, and mass margins data. \n",
    "3. Categories that have different spelling, but the same meaning. \n",
    "\n",
    "As such, the data cleaning tasks are detailed in {numref}`data_cleaning_tasks`:\n",
    "\n",
    "```{table} Data Cleaning Tasks\n",
    ":name: data_cleaning_tasks\n",
    "| # | Task                                                                             |\n",
    "|---|----------------------------------------------------------------------------------|\n",
    "| 1 | Replace invalid values for breast density with NA for imputation.                |\n",
    "| 2 | Replace invalid values for subtlety with NA for imputation            |\n",
    "| 3 | Replace calcification types 'LUCENT_CENTER' with 'LUCENT_CENTERED', and 'PLEOMORPHIC-PLEOMORPHIC', with 'PLEOMORPHIC' |\n",
    "| 4 | Impute missing values using Multiple Imputation by Chained Equations   (MICE)    |\n",
    "| 5 | Conduct random inspection of imputations.                                        |\n",
    "| 6 | Conduct final data quality analysis.                                        |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa1ce5",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if 'jbook' in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../../..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c56d0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from studioai.preprocessing.encode import RankFrequencyEncoder\n",
    "\n",
    "from bcd.data_prep.clean import CBISImputer\n",
    "from bcd.data.dataset import CBISDataset\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19279918",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_STAGED = \"data/meta/2_staged/cbis.csv\"\n",
    "FP_CLEAN = \"data/meta/3_clean/cbis.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d20884",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FP_STAGED)\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661245f9",
   "metadata": {},
   "source": [
    "## Breast Density\n",
    "Replace invalid values for breast density with NA for downstream imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37646c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set invalid values for breast_density to NA\n",
    "df['breast_density'] = df['breast_density'].replace(0, np.NAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f9093",
   "metadata": {},
   "source": [
    "## Subtlety\n",
    "Replace invalid values for subtlety with NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62deb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set case and mass data to NOT APPLICABLE where appropriate.\n",
    "df['subtlety'] = df['subtlety'].replace(0, np.NAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51d615",
   "metadata": {},
   "source": [
    "## Category Alignment\n",
    "Align categories that have the same meaning, but differ in spelling only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07974d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['calc_type'] == 'LUCENT_CENTER', 'calc_type'] = 'LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'ROUND_AND_REGULAR-LUCENT_CENTER-DYSTROPHIC', 'calc_type'] = 'ROUND_AND_REGULAR-LUCENT_CENTERED-DYSTROPHIC'\n",
    "df.loc[df['calc_type'] == 'PUNCTATE-LUCENT_CENTER', 'calc_type'] = 'PUNCTATE-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'VASCULAR-COARSE-LUCENT_CENTER-ROUND_AND_REGULAR-PUNCTATE', 'calc_type'] = 'VASCULAR-COARSE-LUCENT_CENTERED-ROUND_AND_REGULAR-PUNCTATE'\n",
    "df.loc[df['calc_type'] == 'ROUND_AND_REGULAR-LUCENT_CENTER', 'calc_type'] = 'ROUND_AND_REGULAR-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'LUCENT_CENTER-PUNCTATE', 'calc_type'] = 'LUCENT_CENTERED-PUNCTATE'\n",
    "df.loc[df['calc_type'] == 'COARSE-ROUND_AND_REGULAR-LUCENT_CENTER', 'calc_type'] = 'COARSE-ROUND_AND_REGULAR-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'ROUND_AND_REGULAR-LUCENT_CENTER-PUNCTATE', 'calc_type'] = 'ROUND_AND_REGULAR-LUCENT_CENTERED-PUNCTATE'\n",
    "df.loc[df['calc_type'] == 'COARSE-LUCENT_CENTER', 'calc_type'] = 'COARSE-LUCENT_CENTERED'\n",
    "df.loc[df['calc_type'] == 'PLEOMORPHIC-PLEOMORPHIC', 'calc_type'] = 'PLEOMORPHIC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe7dd4",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "Multiple Imputation by Chained Equations (MICE) is a robust, informative method of estimating missing values in datasets. The procedure imputes missing data through an iterative series of predictive models which estimate the value of missing data using the other variables in the dataset. For this, we'll use our CBISImputer which wraps scikit-learn's IterativeImputer implementation of MICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ec9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_mask = df.isnull().any(axis=1)\n",
    "df_missing = df[null_mask]\n",
    "msg = f\"There are {df_missing.shape[0]} rows (approximately {round(df_missing.shape[0] / df_orig.shape[0] * 100,1)}% of the rows) with missing data in the total dataset.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad72b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = CBISImputer(random_state=5)\n",
    "_ = imp.fit(df=df)\n",
    "df_clean = imp.transform(df=df)\n",
    "# Somehow aspect ratio gets corrupted during iterative imputation. IterativeImputer is experimental and the issue of modifying or imputing non-NA values has been raised. Until the issue is isolated and resolved...\n",
    "df_clean[\"aspect_ratio\"] = df_clean[\"cols\"] / df_clean[\"rows\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d056b6",
   "metadata": {},
   "source": [
    "With that, let's save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae966462",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(FP_CLEAN), exist_ok=True)\n",
    "df_clean.to_csv(FP_CLEAN, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f04791",
   "metadata": {},
   "source": [
    "## Random Sample Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d982b02",
   "metadata": {},
   "source": [
    "Let's take a look at a random sampling of the missing data and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37089d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cases = df_missing['mmg_id'].sample(5, random_state=72)\n",
    "df_missing.loc[df_missing['mmg_id'].isin(sample_cases)]\n",
    "df_clean.loc[df_clean['mmg_id'].isin(sample_cases)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb6952",
   "metadata": {},
   "source": [
    "## Data Quality Analysis 2.0\n",
    "Ok, let's have another go at the data quality analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4861df",
   "metadata": {},
   "source": [
    "### Completeness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CBISDataset(filepath=FP_CLEAN)\n",
    "dqcc = ds.dqa.analyze_completeness()\n",
    "print(dqcc.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b378d2",
   "metadata": {},
   "source": [
    "We're complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f8d9b",
   "metadata": {},
   "source": [
    "### Uniqueness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqcu = ds.dqa.analyze_uniqueness()\n",
    "print(dqcu.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef1c4a",
   "metadata": {},
   "source": [
    "We're unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642b775",
   "metadata": {},
   "source": [
    "### Validity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqcv = ds.dqa.analyze_validity()\n",
    "print(dqcv.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5356c",
   "metadata": {},
   "source": [
    "We're valid. That concludes this data cleaning section."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.0"
   }
  },
  "kernelspec": {
   "display_name": "bcd",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   35,
   43,
   60,
   63,
   67,
   70,
   75,
   78,
   83,
   86,
   91,
   102,
   108,
   116,
   122,
   126,
   129,
   133,
   137,
   141,
   146,
   150,
   154,
   158,
   162,
   165,
   169,
   173,
   176
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}